{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries\n",
    "import sqlite3\n",
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "from datetime import datetime as dt\n",
    "import numpy as np\n",
    "from datetime import time as t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sources\n",
    "user_credential = r'\\Users\\dinhhoang.nguyen.CONCENTRIX\\OneDrive - Concentrix Corporation\\WFM-internal'\n",
    "\n",
    "eps_path = os.path.join(\"C:\", user_credential, r\"Data\\EPS Tableau\\EPS Query\")\n",
    "eps_full_path = os.path.join(\"C:\", user_credential, r\"Data\\EPS Tableau\")\n",
    "\n",
    "cpi_path = os.path.join(\"C:\", user_credential, r\"Data\\Ticket\\CPI Query\")\n",
    "cpi_full_path = os.path.join(\"C:\", user_credential, r\"Data\\Ticket\")\n",
    "\n",
    "aht_path = os.path.join(\"C:\", user_credential, r\"Data\\AHT\\AHT query\")\n",
    "aht_full_path = os.path.join(\"C:\", user_credential, r\"Data\\AHT\")\n",
    "\n",
    "csat_path = os.path.join(\"C:\", user_credential, r\"Data\\CSAT RAW\\CSAT Query\")\n",
    "csat_full_path = os.path.join(\"C:\", user_credential, r\"Data\\CSAT RAW\")\n",
    "\n",
    "csatreso_path = os.path.join(\"C:\", user_credential, r\"Data\\CSAT Reso\\CSAT Reso Query\")\n",
    "csatreso_full_path = os.path.join(\"C:\", user_credential, r\"Data\\CSAT Reso\")\n",
    "\n",
    "cuic_path = os.path.join(\"C:\", user_credential, r\"Data\\CUIC\\CUIC Query\")\n",
    "cuic_full_path = os.path.join(\"C:\", user_credential, r\"Data\\CUIC\")\n",
    "\n",
    "exception_path = os.path.join(\"C:\", user_credential, r\"Data\\Exception\")\n",
    "extension_path = os.path.join(\"C:\", user_credential, r\"Data\\Extension\")\n",
    "iex_path = os.path.join(\"C:\", user_credential, r\"Data\\IEX\")\n",
    "label_path = os.path.join(\"C:\", user_credential, r\"Data\\Labels\")\n",
    "ioshrinkage_path = os.path.join(\"C:\", user_credential, r\"Data\\IO Shrinkage\")\n",
    "\n",
    "ot_path = os.path.join(\"C:\", user_credential, r\"Data\\OT\\OT Query\")\n",
    "ot_full_path = os.path.join(\"C:\", user_credential, r\"Data\\OT\")\n",
    "\n",
    "psat_path = os.path.join(\"C:\", user_credential, r\"Data\\PSAT RAW\\PSAT Query\")\n",
    "psat_full_path = os.path.join(\"C:\", user_credential, r\"Data\\PSAT RAW\\PSAT Query\")\n",
    "\n",
    "quality_path = os.path.join(\"C:\", user_credential, r\"Data\\Quality.v2\\Quality query\")\n",
    "quality_full_path = os.path.join(\"C:\", user_credential, r\"Data\\Quality.v2\")\n",
    "\n",
    "ramco_path = os.path.join(\"C:\", user_credential, r\"Data\\Ramco\\Ramco Query\")\n",
    "ramco_full_path = os.path.join(\"C:\", user_credential, r\"Data\\Ramco\")\n",
    "\n",
    "ramcoot_path = os.path.join(\"C:\", user_credential, r\"Data\\Ramco OT\\Ramco OT Query\")\n",
    "ramcoot_full_path = os.path.join(\"C:\", user_credential, r\"Data\\Ramco OT\")\n",
    "\n",
    "requirement_path = os.path.join(\"C:\", user_credential, r\"Data\\Requirement\")\n",
    "\n",
    "schedule_path = os.path.join(\"C:\", user_credential, r\"Data\\Schedule 2\\Schedule Query\")\n",
    "schedule_full_path = os.path.join(\"C:\", user_credential, r\"Data\\Schedule 2\")\n",
    "\n",
    "tl_path = os.path.join(\"C:\", user_credential, r\"Data\\TL\")\n",
    "training_path = os.path.join(\"C:\", user_credential, r\"Data\\Training\")\n",
    "masterroster_path = os.path.join(\"C:\", user_credential, r\"Data\\User\")\n",
    "workplan_path = os.path.join(\"C:\", user_credential, r\"Data\\Workplan\\Workplan Query\")\n",
    "\n",
    "rona_path = os.path.join(\"C:\", user_credential, r\"Data\\RONA daily tracking\\RONA query\")\n",
    "rona_full_path = os.path.join(\"C:\", user_credential, r\"Data\\RONA daily tracking\")\n",
    "\n",
    "workplan_wst_path = os.path.join(\"C:\", user_credential, r\"Data\\Workplan wst\\Workplan wst Query\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_reset = dt(year=2023, month=8, day=1)\n",
    "update_less = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FUNCTION TO IMPORT FOLDERS\n",
    "def import_csv(path):\n",
    "    raw = []\n",
    "    for file in os.listdir(path):\n",
    "        if file.endswith(\".csv\"):\n",
    "            file_path = os.path.join(path, file)\n",
    "            try:\n",
    "                file_data = pd.read_csv(file_path)\n",
    "            except:\n",
    "                file_data = pd.read_csv(file_path, encoding='latin-1')\n",
    "            file_data[\"filename\"] = file\n",
    "            raw.append(file_data)\n",
    "            final_raw = pd.concat(raw, ignore_index=True)\n",
    "    return final_raw\n",
    "    \n",
    "def import_xlsx(path, sheet_name):\n",
    "    raw = []\n",
    "    for file in os.listdir(path):\n",
    "        if file.endswith(\".xlsx\"):\n",
    "            file_path = os.path.join(path, file)\n",
    "            try:\n",
    "                file_data = pd.read_excel(file_path, engine=\"openpyxl\", sheet_name = sheet_name)\n",
    "            except:\n",
    "                print(f\"error with {file_path}\")\n",
    "            file_data[\"filename\"] = file\n",
    "            raw.append(file_data)\n",
    "            final_raw = pd.concat(raw, ignore_index=True)\n",
    "    return final_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to Database\n",
    "conn = sqlite3.connect(os.path.join(r'C:', user_credential, r'DB\\bkn.db'))\n",
    "cursor = conn.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "688"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# IMPORT MASTER ROSTER\n",
    "masterroster = pd.read_excel(os.path.join(r'C:', user_credential, r'Data\\User\\CNX Global Master Roster.xlsx'), sheet_name=\"Sheet1\")\n",
    "masterroster['Employee_ID'] = masterroster['Employee_ID'].astype(\"Int64\")\n",
    "# export master roster to json\n",
    "masterroster['PST_Start_Date'] = pd.to_datetime(masterroster['PST_Start_Date'], format='mixed').dt.strftime('%Y-%m-%d')\n",
    "masterroster['LWD'] = pd.to_datetime(masterroster['LWD'], format='mixed').dt.strftime('%Y-%m-%d')\n",
    "masterroster['Termination_Date'] = pd.to_datetime(masterroster['Termination_Date'], format='mixed').dt.strftime('%Y-%m-%d')\n",
    "masterroster = masterroster.loc[masterroster['Role']=='Agent'] #since Ted  column contains duplicated values (agents and hierarchy have the same names), we have to filter Agent role to have it removed duplicate\n",
    "masterroster.to_sql('masterroster', conn, if_exists='replace', index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "875108"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# IMPORT EPS_full\n",
    "if update_less is False:\n",
    "    eps_full = import_csv(path = eps_full_path)\n",
    "else:\n",
    "    eps_full = import_csv(path = eps_path)\n",
    "\n",
    "eps_full = eps_full.drop(columns=['Index', 'filename', 'Date'])\n",
    "# eps_full['Session Login'] = pd.to_datetime(eps_full['Session Login'], errors='coerce')\n",
    "# eps_full['Session Login'] = eps_full['Session Login'].dt.tz_localize('Europe/Berlin', ambiguous=True).dt.tz_convert('UTC')\n",
    "# eps_full['Session Logout'] = pd.to_datetime(eps_full['Session Logout'], errors='coerce')\n",
    "# eps_full['Session Logout'] = eps_full['Session Logout'].dt.tz_localize('Europe/Berlin', ambiguous=True).dt.tz_convert('UTC')\n",
    " \n",
    "eps_full['Session login VN'] = pd.to_datetime(eps_full['Session Login'], errors='coerce').dt.tz_localize('Europe/Berlin', ambiguous=True).dt.tz_convert('UTC') + pd.Timedelta(hours=7)\n",
    "eps_full['Session logout VN'] = pd.to_datetime(eps_full['Session Logout'], errors='coerce').dt.tz_localize('Europe/Berlin', ambiguous=True).dt.tz_convert('UTC') + pd.Timedelta(hours=7)\n",
    "eps_full['Session login date'] = pd.to_datetime(eps_full['Session login VN']).dt.strftime('%Y-%m-%d')\n",
    "eps_full['Session login time'] = pd.to_datetime(eps_full['Session login VN']).dt.time\n",
    "eps_full['Session logout date'] = pd.to_datetime(eps_full['Session logout VN']).dt.strftime('%Y-%m-%d')\n",
    "eps_full['Session logout time'] = pd.to_datetime(eps_full['Session logout VN']).dt.time\n",
    "\n",
    "eps_full['Session Login'] = pd.to_datetime(eps_full['Session Login']).dt.strftime('%Y-%m-%d %I:%M:%S %p')\n",
    "eps_full['Session Logout'] = pd.to_datetime(eps_full['Session Logout']).dt.strftime('%Y-%m-%d %I:%M:%S %p')\n",
    "eps_full = eps_full.drop(columns=['Session login VN', 'Session logout VN', 'Username', 'manager_username', 'sitecode', 'Session Time'])\n",
    "\n",
    "# export eps_full_full to json\n",
    "eps_full.to_sql('eps', conn, if_exists='replace', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dinhhoang.nguyen.CONCENTRIX\\AppData\\Local\\Temp\\VSCodePortable-x64Temp\\ipykernel_23636\\4180394810.py:8: DtypeWarning: Columns (10,11) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  file_data = pd.read_csv(file_path)\n",
      "C:\\Users\\dinhhoang.nguyen.CONCENTRIX\\AppData\\Local\\Temp\\VSCodePortable-x64Temp\\ipykernel_23636\\4180394810.py:8: DtypeWarning: Columns (10,11) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  file_data = pd.read_csv(file_path)\n",
      "C:\\Users\\dinhhoang.nguyen.CONCENTRIX\\AppData\\Local\\Temp\\VSCodePortable-x64Temp\\ipykernel_23636\\4180394810.py:8: DtypeWarning: Columns (10,11) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  file_data = pd.read_csv(file_path)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1958862"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " #IMPORT AHT\n",
    "if update_less is False:\n",
    "    aht_full = import_csv(path = aht_full_path)\n",
    "else:\n",
    "    aht_full = import_csv(path = aht_path)\n",
    "\n",
    "aht_full = aht_full.drop(columns=['filename'])\n",
    "masterroster_aht_full = masterroster[['Employee_ID', 'TED Name']]\n",
    "aht_full = pd.merge(aht_full, masterroster_aht_full, left_on='Staff', right_on ='TED Name', how='left')\n",
    "aht_full = aht_full.drop(columns=['TED Name'])\n",
    "aht_full['Date'] = pd.to_datetime(aht_full['Date'], errors='coerce')\n",
    "\n",
    "aht_full.to_sql('aht', conn, if_exists='replace', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1958862"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#IMPORT CSAT\n",
    "if update_less is False:\n",
    "    csat = import_csv(path = csat_full_path)\n",
    "else:\n",
    "    csat = import_csv(path = csat_path)\n",
    "\n",
    "masterroster_csat = masterroster[['Employee_ID', 'TED Name']]\n",
    "csat['Date '] = pd.to_datetime(csat['Date '], format='%m/%d/%Y')\n",
    "csat = csat.rename(columns={'Date ': 'Date'})\n",
    "csat = pd.merge(csat, masterroster_csat, left_on='Staff', right_on ='TED Name', how='left')\n",
    "csat = csat.drop(columns=['Sort by Dimension', 'Has Comment', '\"Comment\"', 'Reservation Link', 'View comment', 'Sort by Dimension (copy)', 'filename', 'TED Name', 'Max. Sort by Dimension'])\n",
    "csat['Date'] = csat['Date'].dt.strftime('%Y-%m-%d')\n",
    "# Export to SQL\n",
    "csat.to_sql('csat', conn, if_exists='replace', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21126"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#IMPORT CSAT_RESO\n",
    "if update_less is False:\n",
    "    csat_reso = import_csv(path = csatreso_full_path)\n",
    "else:\n",
    "    csat_reso = import_csv(path = csatreso_path)\n",
    "    \n",
    "masterroster_csat_reso = masterroster[['Employee_ID', 'TED Name']]\n",
    "csat_reso['Date '] = pd.to_datetime(csat_reso['Date '], errors='coerce')\n",
    "csat_reso = csat_reso.rename(columns={'Date ': 'Date'})\n",
    "csat_reso = pd.merge(csat_reso, masterroster_csat_reso, left_on='Staff', right_on ='TED Name', how='left')\n",
    "csat_reso = csat_reso.drop(columns=['Sort by Dimension', 'Has Comment', '\"Comment\"', 'Reservation Link', 'View comment', 'Sort by Dimension (copy)', 'filename', 'TED Name', 'Max. Sort by Dimension'])\n",
    "csat_reso['Date'] = csat_reso['Date'].dt.strftime('%Y-%m-%d')\n",
    "\n",
    "#export csat_reso to sql\n",
    "csat_reso.to_sql('csat_reso', conn, if_exists='replace', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "889180"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# IMPORT RAMCO\n",
    "if update_less is False:\n",
    "    ramco = import_csv(path = ramco_full_path)\n",
    "else:\n",
    "    ramco = import_csv(path = ramco_path)\n",
    "\n",
    "ramco['Attribute'] = pd.to_datetime(ramco['Attribute'], errors='coerce').dt.date\n",
    "ramco=ramco.rename(columns={'Attribute':'Date'})\n",
    "ramco['EID']=ramco['EID'].astype('Int64')\n",
    "\n",
    "# export ramco to json\n",
    "ramco.to_sql('ramco', conn, if_exists='replace', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dinhhoang.nguyen.CONCENTRIX\\AppData\\Local\\Temp\\VSCodePortable-x64Temp\\ipykernel_23636\\2835404172.py:18: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  week_shift['Date'] = pd.to_datetime(week_shift['Date'],errors='coerce')\n",
      "C:\\Users\\dinhhoang.nguyen.CONCENTRIX\\AppData\\Local\\Temp\\VSCodePortable-x64Temp\\ipykernel_23636\\2835404172.py:19: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  week_shift['Week'] = week_shift['Date'].dt.strftime('%Y%W')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "152921"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# IMPORT SCHEDULE\n",
    "if update_less is False:\n",
    "    schedule = import_xlsx(path = schedule_full_path,sheet_name='Sheet1')\n",
    "else:\n",
    "    schedule = import_xlsx(path = schedule_path,sheet_name='Sheet1')\n",
    "\n",
    "schedule['Attribute'] = pd.to_datetime(schedule['Attribute'], errors='coerce').dt.date\n",
    "schedule=schedule.iloc[:,0:8]\n",
    "condition=(schedule['Value']=='OFF')|(schedule['Value']=='AL')|(schedule['Value']=='CO')|(schedule['Value']=='UPL')|(schedule['Value']=='VGH')\n",
    "condition_training=(schedule['Value']=='Training')|(schedule['Value']=='PEGA')\n",
    "schedule['Shift'] = np.where(condition, 'OFF', schedule['NS Check'])\n",
    "schedule['Shift'] = np.where(condition_training, 'Training', schedule['Shift'])\n",
    "schedule=schedule[['Emp ID','Name','Attribute','Value','LOB','Shift']]\n",
    "schedule=schedule.rename(columns={'Attribute':'Date','Value':'Shift','Shift':'Shift_type'})\n",
    "schedule['Emp ID'] = schedule['Emp ID'].astype(\"Int64\")\n",
    " \n",
    "week_shift=schedule[['Emp ID','Date','Shift']]\n",
    "week_shift['Date'] = pd.to_datetime(week_shift['Date'],errors='coerce')\n",
    "week_shift['Week'] = week_shift['Date'].dt.strftime('%Y%W')\n",
    "week_shift=week_shift[['Emp ID','Week','Shift']]\n",
    "week_shift = week_shift[week_shift['Shift'].str.contains('00$')]\n",
    "week_shift=week_shift.groupby(['Emp ID','Week','Shift'],as_index=False).count()\n",
    "week_shift=week_shift.groupby(['Emp ID','Week'],as_index=False).agg(min=('Shift','min'))\n",
    "week_shift=week_shift.rename(columns={'min':'Week_shift'})\n",
    " \n",
    "schedule['Date'] = pd.to_datetime(schedule['Date'],errors='coerce')\n",
    "schedule['Week'] = schedule['Date'].dt.strftime('%Y%W')\n",
    "schedule=pd.merge(schedule,week_shift,left_on=['Emp ID','Week'],right_on=['Emp ID','Week'],how='left')\n",
    "\n",
    "schedule['Date']=schedule['Date'].dt.date\n",
    "\n",
    "schedule=pd.merge(schedule,ramco,left_on=['Emp ID','Date'],right_on=['EID','Date'],how='left')\n",
    "def actualshift_func(x):\n",
    "    if x['Value']=='PH' or x['Value']=='PO' :\n",
    "        return x['Week_shift']\n",
    "    else:         \n",
    "        return x['Shift']\n",
    "schedule['Actual_shift'] = schedule.apply(actualshift_func, axis=1)\n",
    "def actualshift1_func(x):\n",
    "    if x['Actual_shift'] in ['0400-1300','0500-1400','0600-1500','0700-1600','0800-1700','0900-1800','1000-1900','1100-2000',\n",
    "                      '1200-2100','1300-2200','1400-2300','1500-2400','1600-0100','1700-0200', '1800-0300','1900-0400']:     \n",
    "        return 'DS'\n",
    "    elif x['Actual_shift'] in ['2000-0500','2100-0600','2200-0700']:\n",
    "        return 'NS'\n",
    "    else:      \n",
    "        return x['Shift_type']\n",
    "schedule['Actual_shift_type'] = schedule.apply(actualshift1_func, axis=1)\n",
    "schedule=schedule[['Emp ID','Name','Date','Actual_shift','LOB','Actual_shift_type']]\n",
    "schedule=schedule.rename(columns={'Actual_shift':'Shift','Actual_shift_type':'Shift_type'}) # type: ignore\n",
    "\n",
    "# export schedule to json\n",
    "schedule['Date'] = pd.to_datetime(schedule['Date'], errors='coerce').dt.strftime('%Y-%m-%d')\n",
    "schedule.to_sql('schedule', conn, if_exists='replace', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dinhhoang.nguyen.CONCENTRIX\\AppData\\Local\\Temp\\VSCodePortable-x64Temp\\ipykernel_23636\\1492550986.py:7: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  cuic['Date'] = pd.to_datetime(cuic['Interval'], errors='coerce').dt.date\n",
      "C:\\Users\\dinhhoang.nguyen.CONCENTRIX\\AppData\\Local\\Temp\\VSCodePortable-x64Temp\\ipykernel_23636\\1492550986.py:8: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  cuic['Time'] = pd.to_datetime(cuic['Interval'], errors='coerce').dt.time\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1290391"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#IMPORT CUIC\n",
    "if update_less is False:\n",
    "    cuic = import_xlsx(path = cuic_full_path,sheet_name=\"Khoa Interval Attendance Check-\")\n",
    "else:\n",
    "    cuic = import_xlsx(path = cuic_path,sheet_name=\"Khoa Interval Attendance Check-\")\n",
    "\n",
    "cuic['Date'] = pd.to_datetime(cuic['Interval'], errors='coerce').dt.date\n",
    "cuic['Time'] = pd.to_datetime(cuic['Interval'], errors='coerce').dt.time\n",
    "cuic = pd.merge(cuic, masterroster, left_on='LoginName', right_on ='Booking Login ID', how='left')\n",
    "cuic=cuic.iloc[:,0:9]\n",
    "\n",
    "schedule['Date'] = pd.to_datetime(schedule['Date'], errors='coerce').dt.date\n",
    "\n",
    "cuic = pd.merge(cuic, schedule, left_on=['Employee_ID','Date'], right_on =['Emp ID','Date'], how='left')\n",
    "cuic=cuic[['FullName','LoginName','Interval','AgentAvailTime','AgentLoggedOnTime','Date','Time','Employee_ID','Shift_type','Shift']]\n",
    "cuic['Date-1']=pd.to_datetime(cuic['Date'], errors='coerce') - pd.Timedelta(1,\"d\")\n",
    "cuic['Date-1']=pd.to_datetime(cuic['Date-1'], errors='coerce').dt.date\n",
    "cuic = pd.merge(cuic, schedule, left_on=['Employee_ID','Date-1'], right_on =['Emp ID','Date'], how='left')\n",
    "cuic=cuic[['FullName','LoginName','Interval','AgentAvailTime','AgentLoggedOnTime','Date_x','Time','Employee_ID','Shift_type_x','Shift_x','Date-1','Shift_type_y','Shift_y']]\n",
    "mytime=t(12,0,0)\n",
    "condition=(cuic['Shift_type_x']!='DS')&(cuic['Shift_type_y']=='NS')&(cuic['Time']<mytime)\n",
    "cuic['Session Date'] = np.where(condition, cuic['Date-1'], cuic['Date_x'])\n",
    "cuic=cuic[['FullName','LoginName','Interval','AgentAvailTime','AgentLoggedOnTime','Date_x','Employee_ID','Shift_type_x','Session Date']]\n",
    "cuic=cuic.rename(columns={'Date_x':'Date','Shift_type_x':'Shift_type'})\n",
    "cuic['Session Date'] = pd.to_datetime(cuic['Session Date'], errors='coerce').dt.strftime('%Y-%m-%d')\n",
    "# export cuic to json\n",
    "cuic.to_sql('cuic', conn, if_exists='replace', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#EXCEPTION\n",
    "exceptional_hrs = import_xlsx(path= exception_path, sheet_name=\"Sheet1\")\n",
    "exceptional_hrs = exceptional_hrs.rename(columns={'Date \\n(MM/DD/YYYY)':'Date'})\n",
    "exceptional_hrs = exceptional_hrs.drop(columns=['filename'])\n",
    "\n",
    "#export exceptional hours to json\n",
    "exceptional_hrs.to_sql('exceptional_hrs', conn, if_exists='replace', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "630"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#IMPORT IEX ID\n",
    "iex = import_xlsx(path = iex_path, sheet_name=\"Sheet1\")\n",
    "iex = iex[['ID Agent', 'Personal ID']]\n",
    "iex = iex.rename(columns={'Personal ID': 'EID'})\n",
    "\n",
    "#export iex to json\n",
    "iex.to_sql('iex', conn, if_exists='replace', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "636"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# IMPORT TL\n",
    "tl = pd.read_excel(os.path.join(tl_path, \"TL List.xlsx\"),sheet_name='Sheet1')\n",
    "tl=tl.iloc[:,0:6]\n",
    "tl=tl.rename(columns={'Supervisor (no need fill)': 'Supervisor'})\n",
    "tl=tl.drop(columns=['Wave', 'Name', 'LOB', 'Full name (VN)'])\n",
    "\n",
    "# export tl to json\n",
    "tl.to_sql('tl', conn, if_exists='replace', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "182"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# IMPORT TRAINING\n",
    "training = import_xlsx(path = training_path,sheet_name='Sheet1')\n",
    "training['Date'] = pd.to_datetime(training['Date'],format='mixed').dt.date\n",
    "training['EID'] = training['EID'].astype(\"int64\")\n",
    "training=training.iloc[:,1:]\n",
    "\n",
    "#export training to json\n",
    "training.to_sql('training', conn, if_exists='replace', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "451"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# IMPORT IOSHRINKAGE\n",
    "ioshrinkage = import_xlsx(path = ioshrinkage_path, sheet_name='Sheet1')\n",
    "\n",
    "#export ioshrinkage to json\n",
    "ioshrinkage.to_sql('ioshrinkage', conn, if_exists='replace', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "57457"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# IMPORT OT\n",
    "if update_less is False:\n",
    "    ot = import_xlsx(path = ot_full_path, sheet_name='Sheet2')\n",
    "else:\n",
    "    ot = import_xlsx(path = ot_path, sheet_name='Sheet2')\n",
    "ot = ot.drop(columns=['Wave', 'Name', 'Value', 'LOB', 'filename'])\n",
    "ot['Date'] = pd.to_datetime(ot['Date'], errors='coerce')\n",
    "ot['Date'] = ot['Date'].dt.date\n",
    "\n",
    "#export ot to json\n",
    "ot.to_sql('ot', conn, if_exists='replace', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5630"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# IMPORT PSAT\n",
    "if update_less is False:\n",
    "    psat = import_csv(path = psat_full_path)\n",
    "else:\n",
    "    psat = import_csv(path = psat_path)\n",
    "\n",
    "psat['Date'] = pd.to_datetime(psat['Date'],errors='coerce').dt.date\n",
    "\n",
    "#export psat to json\n",
    "psat.to_sql('psat', conn, if_exists='replace', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "466287"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# IMPORT QUALITY\n",
    "if update_less is False:\n",
    "    quality_raw = import_xlsx(path = quality_full_path,sheet_name='Sheet1')\n",
    "else:\n",
    "    quality_raw = import_xlsx(path = quality_path,sheet_name='Sheet1')\n",
    "\n",
    "quality_raw['eval_date'] = pd.to_datetime(quality_raw['eval_date'], errors='coerce').dt.date\n",
    "quality=quality_raw.iloc[:,0:20]\n",
    "quality['eval_date'] = pd.to_datetime(quality['eval_date'], errors='coerce').dt.strftime('%Y-%m-%d')\n",
    "quality['eval_reference'] = quality['eval_id'].astype('str')\n",
    "#export quality to json\n",
    "quality.to_sql('quality', conn, if_exists='replace', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "198095"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# IMPORT RAMCOOT\n",
    "if update_less is False:\n",
    "    ramcoot = import_xlsx(path = ramcoot_full_path,sheet_name='Sheet1')\n",
    "else:\n",
    "    ramcoot = import_xlsx(path = ramcoot_path,sheet_name='Sheet1')\n",
    "\n",
    "ramcoot['Attribute'] = pd.to_datetime(ramcoot['Attribute'],errors='coerce').dt.date\n",
    "ramcoot=ramcoot.rename(columns={'Attribute':'Date'})\n",
    "\n",
    "#export ramcoot to json\n",
    "ramcoot.to_sql('ramcoot', conn, if_exists='replace', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3990"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# IMPORT REQUIREMENT\n",
    "requirement = import_xlsx(path = requirement_path,sheet_name='Sheet1')\n",
    "requirement['Date'] = pd.to_datetime(requirement['Date'],errors='coerce').dt.date\n",
    "\n",
    "#export requirement to json\n",
    "requirement.to_sql('requirement', conn, if_exists='replace', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "236904"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# IMPORT WORKPLAN\n",
    "workplan_raw = import_xlsx(path = workplan_path,sheet_name='Sheet1')\n",
    "workplan_raw['Date'] = pd.to_datetime(workplan_raw['Date'],errors='coerce').dt.date\n",
    "workplan_merge_iex = pd.merge(workplan_raw,iex, left_on='Agent ID', right_on ='ID Agent', how='left')\n",
    "workplan_merge_iex['EID'] = workplan_merge_iex['EID'].astype(\"Int64\")\n",
    "workplan_merge_iex=workplan_merge_iex.drop(columns=['filename', 'ID Agent'])\n",
    "workplan_merge_iex = workplan_merge_iex[['LOB', 'Date', 'Agent ID', 'Agent Name', 'Scheduled Activity', 'Length', 'Percent', 'EID']]\n",
    "#export workplan_merge_iex to json\n",
    "workplan_merge_iex.to_sql('workplan_merge_iex', conn, if_exists='replace', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "34"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#IMPORT LABELS\n",
    "labels = import_csv(label_path)\n",
    "\n",
    "#export labels to json\n",
    "labels.to_sql('labels', conn, if_exists='replace', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2625"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kpi_target = pd.read_excel(os.path.join(r'C:', user_credential, r'Data\\KPI Target\\kpi_target.xlsx'), engine=\"openpyxl\", sheet_name = 'Sheet1')\n",
    "kpi_target.to_sql('kpi_target', conn, if_exists='replace', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "372597"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#IMPORT FULL CPI_full\n",
    "if update_less is False:\n",
    "    cpi_full = import_csv(path = cpi_full_path)\n",
    "else:\n",
    "    cpi_full = import_csv(path = cpi_path)\n",
    "    \n",
    "cpi_full = cpi_full.drop(columns=['Region Partner', 'Sitecode', 'Mp Csm Name', 'Mp Team Name'])\n",
    "cpi_full['Date'] = cpi_full['filename'].replace(\".csv\", \"\", regex=True)\n",
    "cpi_full['Date'] = pd.to_datetime(cpi_full['Date'], errors='coerce')\n",
    "\n",
    "masterroster_cpi_full = masterroster[['Employee_ID', 'TED Name']]\n",
    "cpi_full = pd.merge(cpi_full, masterroster_cpi_full, left_on=\"Staff Name\", right_on=\"TED Name\", how=\"left\")\n",
    "cpi_full = cpi_full.drop(columns=['filename', 'TED Name'])\n",
    "cpi_full = cpi_full[['Staff Name', 'Employee_ID', 'Date', 'Channel', 'Number of Records']]\n",
    "\n",
    "# export cpi_full to json\n",
    "cpi_full.to_sql('cpi_full', conn, if_exists='replace', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dinhhoang.nguyen.CONCENTRIX\\AppData\\Local\\Temp\\VSCodePortable-x64Temp\\ipykernel_23636\\2550752670.py:10: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  rona['DateTime'] = pd.to_datetime(rona['DateTime'], errors='coerce') + pd.Timedelta(hours=5)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1194859"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# IMPORT RONA\n",
    "if update_less is False:\n",
    "    rona = import_xlsx(path = rona_full_path,sheet_name='RTA-Agent Team Historical All F')\n",
    "else:\n",
    "    rona = import_xlsx(path = rona_path,sheet_name='RTA-Agent Team Historical All F')\n",
    "\n",
    "## extract only 3 cols: Agent, Datetime and RONA\n",
    "rona = rona[['Agent', 'DateTime', 'RONA']]\n",
    "## convert time to VN Time\n",
    "rona['DateTime'] = pd.to_datetime(rona['DateTime'], errors='coerce') + pd.Timedelta(hours=5)\n",
    "## here we create 2 date and time time cols from DateTime\n",
    "rona['Date'] = pd.to_datetime(rona['DateTime'], errors='coerce').dt.date\n",
    "rona['Time'] = pd.to_datetime(rona['DateTime'], errors='coerce').dt.time\n",
    "## we get the ID from CUIC Name for us to easier manipulating the data\n",
    "rona = pd.merge(rona, masterroster[['CUIC Name', 'Employee_ID']], left_on='Agent', right_on='CUIC Name', how='left')\n",
    "rona = rona.drop(columns={'CUIC Name'})\n",
    "rona['Employee_ID'] = rona['Employee_ID'].dropna()\n",
    "## Get the Session date\n",
    "schedule['Date'] = pd.to_datetime(schedule['Date'], errors='coerce').dt.date\n",
    "rona = pd.merge(rona, schedule[['Emp ID','Date', 'Shift_type']], left_on=['Employee_ID','Date'], right_on =['Emp ID','Date'], how='left')\n",
    "rona = rona.drop(columns={'Emp ID'})\n",
    "rona['Date-1']=pd.to_datetime(rona['Date'], errors='coerce') - pd.Timedelta(1,\"d\")\n",
    "rona['Date-1']=pd.to_datetime(rona['Date-1'], errors='coerce').dt.date\n",
    "rona = pd.merge(rona, schedule[['Emp ID', 'Date', 'Shift_type']], left_on=['Employee_ID','Date-1'], right_on =['Emp ID','Date'], how='left')\n",
    "mytime=t(12,0,0)\n",
    "condition=(rona['Shift_type_x']!='DS')&(rona['Shift_type_y']=='NS')&(rona['Time']<mytime)\n",
    "rona['Session Date'] = np.where(condition, rona['Date-1'], rona['Date_x'])\n",
    "rona = rona.drop(columns={'Date_y', 'Shift_type_x', 'Shift_type_y', 'Emp ID', 'Date-1'})\n",
    "rona = rona.rename(columns={'Date_x': 'Date'})\n",
    "rona['Session Date'] = pd.to_datetime(rona['Session Date'], errors='coerce').dt.strftime('%Y-%m-%d')\n",
    "## extract to json\n",
    "rona.to_sql('rona', conn, if_exists='replace', index=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "128963"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# IMPORT WORKPLAN WITH START TIME\n",
    "workplan_wst = import_xlsx(path=workplan_wst_path, sheet_name='Sheet1')\n",
    "# remove rows where there's no activity\n",
    "workplan_wst = workplan_wst.loc[workplan_wst['Start2'].notna()]\n",
    "\n",
    "workplan_wst['Date_end'] = workplan_wst.apply(lambda x: pd.to_datetime(x['Date'], errors='coerce') + pd.Timedelta(days=1) if x['Start1'] >= t(20, 0, 0) else pd.to_datetime(x['Date'], errors='coerce'), axis=1)\n",
    "workplan_wst['date_starttime'] = pd.to_datetime(workplan_wst['Date'], errors='coerce').dt.strftime('%Y-%m-%d') + \" \" + workplan_wst['Start1'].astype('str')\n",
    "workplan_wst['date_starttime'] = pd.to_datetime(workplan_wst['date_starttime'], errors='coerce')\n",
    "workplan_wst['date_endtime'] = pd.to_datetime(workplan_wst['Date_end'], errors='coerce').dt.strftime('%Y-%m-%d') + \" \" + workplan_wst['End1'].astype('str')\n",
    "workplan_wst['date_endtime'] = pd.to_datetime(workplan_wst['date_endtime'], errors='coerce')\n",
    "\n",
    "workplan_wst['date_act_start'] = workplan_wst.apply(lambda x: pd.to_datetime(x['Date'], errors='coerce') + pd.Timedelta(days=1) if x['Start1'] >= t(20, 0, 0) and x['Start2'] <= t(15, 0, 0) else pd.to_datetime(x['Date'], errors='coerce'),axis=1)\n",
    "workplan_wst['act_starttime'] = pd.to_datetime(workplan_wst['date_act_start'], errors='coerce').dt.strftime('%Y-%m-%d') + \" \" + workplan_wst['Start2'].astype('str')\n",
    "workplan_wst['act_starttime'] = pd.to_datetime(workplan_wst['act_starttime'], errors='coerce')\n",
    "workplan_wst['date_act_end'] = workplan_wst.apply(lambda x: pd.to_datetime(x['Date'], errors='coerce') + pd.Timedelta(days=1) if x['Start1'] >= t(20, 0, 0) and x['End2'] <= t(15, 0, 0) else pd.to_datetime(x['Date'], errors='coerce'),axis=1)\n",
    "workplan_wst['act_endtime'] = pd.to_datetime(workplan_wst['date_act_end'], errors='coerce').dt.strftime('%Y-%m-%d') + \" \" + workplan_wst['End2'].astype('str')\n",
    "workplan_wst['act_endtime'] = pd.to_datetime(workplan_wst['act_endtime'], errors='coerce')\n",
    "\n",
    "# date_duration show the total time that the agents are workplanned to work in a day\n",
    "workplan_wst['date_duration'] = (pd.to_datetime(workplan_wst['date_endtime'], format='%Y-%m-%d%H:%M:%S', errors='coerce') - pd.to_datetime(workplan_wst['date_starttime'], format='%Y-%m-%d%H:%M:%S', errors='coerce'))\n",
    "workplan_wst['date_duration'] = workplan_wst.apply(lambda x: x['date_duration'].total_seconds()/86400, axis=1)\n",
    "\n",
    "# act_duration show the total time that the agents are workplanned to work in a day\n",
    "workplan_wst['act_duration'] = (pd.to_datetime(workplan_wst['act_endtime'], format='%Y-%m-%d%H:%M:%S', errors='coerce') - pd.to_datetime(workplan_wst['act_starttime'], format='%Y-%m-%d%H:%M:%S', errors='coerce'))\n",
    "workplan_wst['act_duration'] = workplan_wst.apply(lambda x: x['act_duration'].total_seconds()/86400, axis=1)\n",
    "\n",
    "# Trim the table\n",
    "workplan_wst = workplan_wst[['LOB', 'ID', 'Date', 'date_starttime', 'date_endtime', 'Schedule Act','act_starttime','act_endtime', 'date_duration', 'act_duration']]\n",
    "# Rename ID to Agent ID\n",
    "workplan_wst = workplan_wst.rename(columns={'ID': 'Agent ID', 'act_duration': 'Length'})\n",
    "\n",
    "# Get ID (CNX ID)\n",
    "workplan_wst = pd.merge(workplan_wst,iex, left_on='Agent ID', right_on ='ID Agent', how='left')\n",
    "workplan_wst = workplan_wst.drop(columns={'ID Agent'})\n",
    "\n",
    "# convert datetime value to string and export to json\n",
    "workplan_wst['Date'] = pd.to_datetime(workplan_wst['Date'], errors='coerce').dt.strftime('%Y-%m-%d')\n",
    "workplan_wst['date_starttime']=pd.to_datetime(workplan_wst['date_starttime'], errors='coerce').dt.strftime('%Y-%m-%d %I:%M:%S %p')\n",
    "workplan_wst['date_endtime']=pd.to_datetime(workplan_wst['date_endtime'], errors='coerce').dt.strftime('%Y-%m-%d %I:%M:%S %p')\n",
    "workplan_wst['act_starttime'] = pd.to_datetime(workplan_wst['act_starttime'], errors='coerce').dt.strftime('%Y-%m-%d %I:%M:%S %p')\n",
    "workplan_wst['act_endtime'] = pd.to_datetime(workplan_wst['act_endtime'], errors='coerce').dt.strftime('%Y-%m-%d %I:%M:%S %p')\n",
    "\n",
    "workplan_wst.to_sql('workplan_wst', conn, if_exists='replace', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn.commit()\n",
    "conn.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
