{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#libraries\n",
    "import os\n",
    "import pandas as pd\n",
    "from datetime import datetime as dt\n",
    "from datetime import time as t\n",
    "from dateutil import relativedelta\n",
    "import numpy as np\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_reset = dt(year=2023, month=10, day=1)\n",
    "update_less = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# source\n",
    "personal_path = os.environ['USERPROFILE']\n",
    "if personal_path in ['C:\\\\Users\\\\dinhhoang.nguyen.CONCENTRIX', 'C:\\\\Users\\\\ADMIN']:\n",
    "    middle_path = r'OneDrive - Concentrix Corporation\\WFM-internal'\n",
    "else:\n",
    "    middle_path = r'Concentrix Corporation\\Dinh Hoang Nguyen - WFM-internal'\n",
    "user_credential = os.path.join(os.environ['USERPROFILE'], middle_path)\n",
    "\n",
    "masterroster_path = os.path.join(user_credential, r'DB\\filejson\\masterroster.json')\n",
    "eps_path = os.path.join(user_credential, r'DB\\filejson\\eps.json')\n",
    "eps_full_path = os.path.join(user_credential, r'DB\\filejson\\eps_full.json')\n",
    "cpi_path = os.path.join(user_credential, r'DB\\filejson\\cpi.json')\n",
    "cpi_full_path = os.path.join(user_credential, r'DB\\filejson\\cpi_full.json')\n",
    "aht_path = os.path.join(user_credential, r'DB\\filejson\\aht.json')\n",
    "aht_full_path = os.path.join(user_credential, r'DB\\filejson\\aht_full.json')\n",
    "schedule_path = os.path.join(user_credential, r'DB\\filejson\\schedule.json')\n",
    "schedule_full_path = os.path.join(user_credential, r'DB\\filejson\\schedule_full.json')\n",
    "cuic_path = os.path.join(user_credential, r'DB\\filejson\\cuic.json')\n",
    "cuic_full_path = os.path.join(user_credential, r'DB\\filejson\\cuic_full.json')\n",
    "tl_path = os.path.join(user_credential, r'DB\\filejson\\tl.json')\n",
    "quality_path = os.path.join(user_credential, r'DB\\filejson\\quality.json')\n",
    "quality_full_path = os.path.join(user_credential, r'DB\\filejson\\quality_full.json')\n",
    "storage_path = os.path.join(user_credential, r'DB\\filejson\\kpi_storage.json')\n",
    "output_csv_path = os.path.join(user_credential, r'DB\\filecsv\\[BcomDB] KPI.csv')\n",
    "output_tony_path = os.path.join(personal_path,r'Concentrix Corporation\\Tung Quan Le - BKN\\AgentDetail\\[BcomDB] KPI.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORT MASTER ROSTER\n",
    "masterroster = pd.read_json(masterroster_path)\n",
    "masterroster['Employee_ID'] = masterroster['Employee_ID'].astype(\"Int64\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORT EPS\n",
    "if update_less is True:\n",
    "    eps = pd.read_json(eps_path)\n",
    "else:\n",
    "    eps = pd.read_json(eps_full_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#IMPORT CPI\n",
    "if update_less is True:\n",
    "    cpi = pd.read_json(cpi_path)\n",
    "else:\n",
    "    cpi = pd.read_json(cpi_full_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#IMPORT AHT\n",
    "if update_less is True:\n",
    "    aht = pd.read_json(aht_path)\n",
    "else:\n",
    "    aht = pd.read_json(aht_full_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORT SCHEDULE\n",
    "if update_less is True:\n",
    "    schedule = pd.read_json(schedule_path)\n",
    "else:\n",
    "    schedule = pd.read_json(schedule_full_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORT CUIC\n",
    "if update_less is True:\n",
    "    cuic = pd.read_json(cuic_path)\n",
    "else:\n",
    "    cuic = pd.read_json(cuic_full_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORT TL\n",
    "tl = pd.read_json(tl_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# quality\n",
    "if update_less == True:\n",
    "    quality = pd.read_json(quality_path)\n",
    "else:\n",
    "    quality = pd.read_json(quality_full_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "schedule['Date'] = schedule['Date'].dt.date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"['team_leader'] not in index\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Frame\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m main_frame \u001b[38;5;241m=\u001b[39m \u001b[43mschedule\u001b[49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mEmp ID\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mName\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mDate\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mShift\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mLOB\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mShift_type\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mteam_leader\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m      3\u001b[0m main_frame \u001b[38;5;241m=\u001b[39m main_frame\u001b[38;5;241m.\u001b[39mrename(columns\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mteam_leader\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSupervisor\u001b[39m\u001b[38;5;124m'\u001b[39m})\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m## add Week\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ADMIN\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\frame.py:3899\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3897\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n\u001b[0;32m   3898\u001b[0m         key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(key)\n\u001b[1;32m-> 3899\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_indexer_strict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcolumns\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m   3901\u001b[0m \u001b[38;5;66;03m# take() does not accept boolean indexers\u001b[39;00m\n\u001b[0;32m   3902\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(indexer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\ADMIN\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:6115\u001b[0m, in \u001b[0;36mIndex._get_indexer_strict\u001b[1;34m(self, key, axis_name)\u001b[0m\n\u001b[0;32m   6112\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   6113\u001b[0m     keyarr, indexer, new_indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reindex_non_unique(keyarr)\n\u001b[1;32m-> 6115\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_raise_if_missing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeyarr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   6117\u001b[0m keyarr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtake(indexer)\n\u001b[0;32m   6118\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Index):\n\u001b[0;32m   6119\u001b[0m     \u001b[38;5;66;03m# GH 42790 - Preserve name from an Index\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ADMIN\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:6179\u001b[0m, in \u001b[0;36mIndex._raise_if_missing\u001b[1;34m(self, key, indexer, axis_name)\u001b[0m\n\u001b[0;32m   6176\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNone of [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] are in the [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maxis_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   6178\u001b[0m not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(ensure_index(key)[missing_mask\u001b[38;5;241m.\u001b[39mnonzero()[\u001b[38;5;241m0\u001b[39m]]\u001b[38;5;241m.\u001b[39munique())\n\u001b[1;32m-> 6179\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnot_found\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not in index\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mKeyError\u001b[0m: \"['team_leader'] not in index\""
     ]
    }
   ],
   "source": [
    "# Frame\n",
    "main_frame = schedule[['Emp ID', 'Name', 'Date', 'Shift', 'LOB', 'Shift_type', 'team_leader']]\n",
    "main_frame = main_frame.rename(columns={'team_leader': 'Supervisor'})\n",
    "\n",
    "## add Week\n",
    "main_frame['Date'] = pd.to_datetime(main_frame['Date'], errors='coerce')\n",
    "main_frame['Week'] = main_frame['Date'].dt.strftime('%Y%W')\n",
    "main_frame['Date'] = main_frame['Date'].dt.date\n",
    "\n",
    "## Add LOB Group\n",
    "lob_map = {'EN': 'English', 'VICSP': 'Vietnamese CSP', 'VICSG': 'Vietnamese CSG', 'Senior VICSP': 'Senior CSP', 'NL': 'Unbabel', 'ID4': 'Unbabel', 'HE4': 'Unbabel', 'XT4': 'Unbabel', 'EL': 'Unbabel', 'TR': 'Unbabel', 'KO': 'Unbabel', 'UB CS': 'Unbabel', 'HU': 'Unbabel', 'FR': 'Unbabel', 'ZH': 'Unbabel', 'RU': 'Unbabel', 'UB PL': 'Unbabel', 'PT': 'Unbabel'}\n",
    "## add tenure\n",
    "main_frame['LOB Group'] = main_frame['LOB'].map(lob_map)\n",
    "\n",
    "main_frame = pd.merge(main_frame, masterroster[['Employee_ID', 'Booking Login ID', 'TED Name', 'CUIC Name', 'PST_Start_Date', 'Wave #', 'Role']], left_on='Emp ID', right_on='Employee_ID', how='left')\n",
    "main_frame['PST_Start_Date'] = pd.to_datetime(main_frame['PST_Start_Date'], format='mixed').dt.date\n",
    "def TN(x):\n",
    "    if x['PST_Start_Date'] is pd.NaT:\n",
    "        return 'unknown'\n",
    "    else:\n",
    "        if x['Date'] - x['PST_Start_Date'] >= pd.Timedelta(days=90):\n",
    "            return \"TN\"\n",
    "        else:\n",
    "            return \"NH\"\n",
    "    \n",
    "def tenure_days(x):\n",
    "    if x['PST_Start_Date'] is pd.NaT:\n",
    "        return 'unknown'\n",
    "    else:\n",
    "        days = x['Date'] - x['PST_Start_Date']\n",
    "        if days <= pd.Timedelta(days=30):\n",
    "            return '0-30'\n",
    "        elif days <= pd.Timedelta(days=60):\n",
    "            return '31-60'\n",
    "        elif days <= pd.Timedelta(days=90):\n",
    "            return '61-90'\n",
    "        elif days <= pd.Timedelta(days=120):\n",
    "            return '91-120'\n",
    "        else:\n",
    "            return '120+'\n",
    "    \n",
    "main_frame['Tenure'] = main_frame.apply(TN, axis=1)\n",
    "main_frame['Tenure days'] = main_frame.apply(tenure_days, axis=1)\n",
    "main_frame = main_frame.drop(columns=['Employee_ID'])\n",
    "## add supervisor\n",
    "# main_frame = pd.merge(main_frame, tl[['Emp ID', 'Supervisor']], on='Emp ID', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## cpi processing\n",
    "aht_cpi = cpi\n",
    "\n",
    "## add phone cases\n",
    "aht_cpi['#phone'] = aht_cpi.apply(lambda x: x['Number of Records'] if x['Channel'] == 'phone' else 0, axis=1)\n",
    "aht_cpi['#non-phone'] = aht_cpi.apply(lambda x: x['Number of Records'] if x['Channel'] != 'phone' else 0, axis=1)\n",
    "\n",
    "aht_cpi = aht_cpi.groupby(['Employee_ID', 'Date'], as_index=False)[['#phone', '#non-phone']].sum()\n",
    "aht_cpi['total_cases'] = aht_cpi['#phone'] + aht_cpi['#non-phone']\n",
    "### add to main frame\n",
    "aht_cpi['Date'] = aht_cpi['Date'].dt.date\n",
    "main_frame = pd.merge(main_frame, aht_cpi, left_on=['Emp ID', 'Date'], right_on=['Employee_ID', 'Date'], how='left')\n",
    "main_frame = main_frame.drop(columns={'Employee_ID'})\n",
    "main_frame[['#phone', '#non-phone', 'total_cases']] = main_frame[['#phone', '#non-phone', 'total_cases']].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## AHT\n",
    "aht_aht = aht.drop(columns=['Start Time', 'End Time', '\"Handling Time\"', 'First Email Id'])\n",
    "\n",
    "# add Eliminate Phone\n",
    "def eliminate_phone(x):\n",
    "    if x['Item Channel'] =='Phone' and x['Topic'] =='Unknown':\n",
    "        return \"Notphone\"\n",
    "    else:\n",
    "        return x['Item Channel']\n",
    "aht_aht['Eliminate Phone'] = aht_aht.apply(eliminate_phone, axis=1)\n",
    "\n",
    "# aht_mr = masterroster[['Employee_ID', 'PST_Start_Date', 'CUIC Name', 'Full name', 'Role', 'TED Name', 'Wave #']]\n",
    "\n",
    "# aht_aht = pd.merge(aht_aht, aht_mr, on='Employee_ID', how='left')\n",
    "\n",
    "# aht_aht = pd.merge(aht_aht, tl, left_on ='Employee_ID_x', right_on = 'Emp ID', how='left')\n",
    "# aht_lob = schedule[['Emp ID', 'Date', 'LOB']]\n",
    "# aht_lob['Date'] = pd.to_datetime(aht_lob['Date'], format ='mixed')\n",
    "# aht_aht = pd.merge(aht_aht, aht_lob, left_on = ['Employee_ID', 'Date'], right_on=['Emp ID', 'Date'], how='left')\n",
    "# aht_aht = aht_aht.drop(columns=['Employee_ID_y', 'Emp ID_x', 'Emp ID_y'])\n",
    "# aht_aht = aht_aht.rename(columns={'Employee_ID_x':'Employee_ID'})\n",
    "#Add talk time\n",
    "def talk_time(x):\n",
    "    try:\n",
    "        from_left = 11\n",
    "        to_right = str(x['Tooltip Phone Time']).find(' Wrap Time')\n",
    "        return int(x['Tooltip Phone Time'][from_left:to_right])\n",
    "    except:\n",
    "        return 0\n",
    "def wrap_time(x):\n",
    "    try:\n",
    "        from_left = int(str(x['Tooltip Phone Time']).find('Wrap Time'))+11\n",
    "        to_right = str(x['Tooltip Phone Time']).find(' Hold Time')\n",
    "        return int(x['Tooltip Phone Time'][from_left:to_right])\n",
    "    except:\n",
    "        return 0\n",
    "def hold_time(x):\n",
    "    try:\n",
    "        from_left = int(str(x['Tooltip Phone Time']).find('Hold Time'))+10\n",
    "        return int(x['Tooltip Phone Time'][from_left:])\n",
    "    except:\n",
    "        return 0    \n",
    "aht_aht['Talk Time'] = aht_aht.apply(talk_time, axis=1)\n",
    "aht_aht['Wrap Time'] = aht_aht.apply(wrap_time, axis=1)\n",
    "aht_aht['Hold Time'] = aht_aht.apply(hold_time, axis=1)\n",
    "\n",
    "def act_handling_time(x):\n",
    "    if x['Eliminate Phone'] =='Phone':\n",
    "        return x['Talk Time'] + x['Wrap Time'] + x['Hold Time']\n",
    "    else:\n",
    "        return x['Handling Time']\n",
    "\n",
    "# add realchannel\n",
    "def realchannel(x):\n",
    "    if x['Item Channel'] == 'Phone' and x['Topic'] == 'Unknown':\n",
    "        return \"Phone1\"\n",
    "    else:\n",
    "        return x['Item Channel']\n",
    "aht_aht['Real Channel'] = aht_aht.apply(realchannel, axis=1)\n",
    "\n",
    "# add total inbound\n",
    "def totalinbound(x):\n",
    "    if x['Item Channel'] in ['Email', 'Live Chat', 'Messaging', 'Unknown']:\n",
    "        return x['Handling Time']\n",
    "    else:\n",
    "        return x['Talk Time'] + x['Wrap Time'] + x['Hold Time']\n",
    "\n",
    "aht_aht['Total Inbound'] = aht_aht.apply(totalinbound, axis=1)\n",
    "\n",
    "#add Act handling time\n",
    "aht_aht['Act Handling Time'] = aht_aht.apply(act_handling_time, axis=1)\n",
    "#Date to_datetime\n",
    "aht_aht['Date'] = pd.to_datetime(aht_aht['Date'], format='mixed').dt.strftime('%Y-%m-%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# time & count for AHT\n",
    "aht_aht['AHT Phone time'] = aht_aht.apply(lambda x: x['Act Handling Time'] if x['Eliminate Phone'] == 'Phone' else 0, axis=1)\n",
    "aht_aht['AHT Phone count'] = aht_aht.apply(lambda x: 1 if x['Eliminate Phone'] == 'Phone' else 0, axis=1)\n",
    "\n",
    "aht_aht['AHT Non-phone time'] = aht_aht.apply(lambda x: x['Act Handling Time'] if x['Eliminate Phone'] in ['Email', 'Unknown', 'Messaging', 'SNR', 'BH'] else 0, axis=1)\n",
    "aht_aht['AHT Non-phone count'] = aht_aht.apply(lambda x: 1 if x['Eliminate Phone'] in ['Email', 'Unknown', 'Messaging', 'SNR', 'BH'] else 0, axis=1)\n",
    "\n",
    "aht_aht['Overall AHT time'] = aht_aht.apply(lambda x: x['Act Handling Time'] if x['Eliminate Phone'] != 'Notphone' else 0, axis=1)\n",
    "aht_aht['Overall AHT count'] = aht_aht.apply(lambda x: 1 if x['Eliminate Phone'] != 'Notphone' else 0, axis=1)\n",
    "\n",
    "aht_aht['Hold (phone) time'] = aht_aht.apply(lambda x: x['Hold Time'] if x['Eliminate Phone'] == 'Phone' else 0, axis=1)\n",
    "aht_aht['Hold (phone) count'] = aht_aht.apply(lambda x: 1 if x['Eliminate Phone'] == 'Phone' else 0, axis=1)\n",
    "\n",
    "aht_aht['AACW (phone) time'] = aht_aht.apply(lambda x: x['Wrap Time'] if x['Eliminate Phone'] == 'Phone' else 0, axis=1)\n",
    "aht_aht['AACW (phone) count'] = aht_aht.apply(lambda x: 1 if x['Eliminate Phone'] == 'Phone' else 0, axis=1)\n",
    "\n",
    "aht_aht['Avg Talk Time time'] = aht_aht.apply(lambda x: x['Talk Time'] if x['Eliminate Phone'] == 'Phone' else 0, axis=1)\n",
    "aht_aht['Avg Talk Time count'] = aht_aht.apply(lambda x: 1 if x['Eliminate Phone'] == 'Phone' else 0, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# group by AHT\n",
    "aht_aht = aht_aht.groupby(['Date', 'Employee_ID'], as_index=False)[['AHT Phone time', 'AHT Phone count', 'AHT Non-phone time', 'AHT Non-phone count', 'Overall AHT time', 'Overall AHT count', 'Hold (phone) time', 'Hold (phone) count', 'AACW (phone) time', 'AACW (phone) count', 'Avg Talk Time time', 'Avg Talk Time count']].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add aht_aht to main frame\n",
    "main_frame['Date'] = pd.to_datetime(main_frame['Date'], format='mixed').dt.strftime('%Y-%m-%d')\n",
    "main_frame = pd.merge(main_frame, aht_aht, left_on =['Emp ID', 'Date'], right_on=['Employee_ID', 'Date'], how='left')\n",
    "main_frame = main_frame.drop(columns={'Employee_ID'})\n",
    "main_frame['AHT Phone time'] = main_frame['AHT Phone time'].fillna(0)\n",
    "main_frame['AHT Phone count'] = main_frame['AHT Phone count'].fillna(0)\n",
    "\n",
    "main_frame['AHT Non-phone time'] = main_frame['AHT Non-phone time'].fillna(0)\n",
    "main_frame['AHT Non-phone count'] = main_frame['AHT Non-phone count'].fillna(0)\n",
    "\n",
    "main_frame['Overall AHT time'] = main_frame['Overall AHT time'].fillna(0)\n",
    "main_frame['Overall AHT count'] = main_frame['Overall AHT count'].fillna(0)\n",
    "\n",
    "main_frame['Hold (phone) time'] = main_frame['Hold (phone) time'].fillna(0)\n",
    "main_frame['Hold (phone) count'] = main_frame['Hold (phone) count'].fillna(0)\n",
    "\n",
    "main_frame['AACW (phone) time'] = main_frame['AACW (phone) time'].fillna(0)\n",
    "main_frame['AACW (phone) count'] = main_frame['AACW (phone) count'].fillna(0)\n",
    "\n",
    "main_frame['Avg Talk Time time'] = main_frame['Avg Talk Time time'].fillna(0)\n",
    "main_frame['Avg Talk Time count'] = main_frame['Avg Talk Time count'].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cuic\n",
    "aht_cuic = cuic\n",
    "aht_cuic = aht_cuic.groupby(['Employee_ID', 'Session Date'], as_index=False)[['AgentAvailTime']].sum()\n",
    "aht_cuic['AgentAvailTime']=aht_cuic['AgentAvailTime']*24\n",
    "aht_cuic['Session Date'] = pd.to_datetime(aht_cuic['Session Date'], format='mixed').dt.strftime('%Y-%m-%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add cuic to main frame\n",
    "main_frame = pd.merge(main_frame, aht_cuic, left_on =['Emp ID', 'Date'], right_on=['Employee_ID', 'Session Date'], how='left')\n",
    "main_frame = main_frame.drop(columns={'Employee_ID', 'Session Date'})\n",
    "main_frame['AgentAvailTime'] = main_frame['AgentAvailTime'].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# eps\n",
    "aht_eps = eps\n",
    "# calculate time\n",
    "aht_eps['phone_time'] = aht_eps.apply(lambda x: x['Total Time'] if x['BPE Code'] in ['Ready or Talking','Finalize Call'] else 0, axis=1)/3600\n",
    "aht_eps['nonphone_time'] = aht_eps.apply(lambda x: x['Total Time'] if x['BPE Code'] == 'Picklist - off Phone' else 0, axis=1)/3600\n",
    "aht_eps['productive_time'] = aht_eps.apply(lambda x: x['Total Time'] if x['BPE Code'] in ['Picklist - off Phone', 'Ready or Talking''Finalize Call','RONA','Unscheduled Picklist','Payment Processing','Social Media','Mass Issue','Project'] else 0, axis=1)/3600\n",
    "\n",
    "## add session date\n",
    "aht_eps['Session login date'] = pd.to_datetime(aht_eps['Session login date'], format='mixed').dt.date\n",
    "aht_eps['Session login date-1'] = aht_eps['Session login date'] - pd.Timedelta(days=1)\n",
    "aht_eps = pd.merge(aht_eps, schedule[['Emp ID', 'Date', 'Shift_type']], left_on=['EID', 'Session login date-1'], right_on=['Emp ID', 'Date'], how='left')\n",
    "\n",
    "def session_date(x):\n",
    "    if x['Shift_type'] == 'NS' and x['Session login time'] <= t(12, 0, 0):\n",
    "        return x['Session login date-1']\n",
    "    else:\n",
    "        return x['Session login date']\n",
    "    \n",
    "aht_eps['Session login time'] = pd.to_datetime(aht_eps['Session login time'], format='mixed').dt.time\n",
    "aht_eps['Session date'] = aht_eps.apply(session_date, axis=1)\n",
    "\n",
    "aht_eps = aht_eps.groupby(['EID', 'Session date'], as_index=False)[['phone_time', 'nonphone_time', 'productive_time']].sum()\n",
    "aht_eps = aht_eps.rename(columns={'Session date': 'Date'})\n",
    "\n",
    "aht_eps['Date'] = pd.to_datetime(aht_eps['Date'], format = 'mixed').dt.strftime('%Y-%m-%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## add aht_eps to main frame\n",
    "main_frame = pd.merge(main_frame, aht_eps, left_on =['Emp ID', 'Date'], right_on=['EID', 'Date'], how='left')\n",
    "main_frame = main_frame.drop(columns={'EID'})\n",
    "main_frame[['phone_time', 'nonphone_time', 'productive_time']] = main_frame[['phone_time', 'nonphone_time', 'productive_time']].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_frame['Date'] = pd.to_datetime(main_frame['Date'], format='mixed').dt.strftime('%Y-%m-%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quality = quality.rename(columns={' score_question_weight': 'score_question_weight'})\n",
    "quality[['score_n', 'score_question_weight']] = quality[['score_n', 'score_question_weight']].fillna(0)\n",
    "\n",
    "quality['customer_score'] = quality.apply(lambda x: x['score_n'] if x['sections'] == 'CUSTOMER' else 0, axis=1)\n",
    "quality['customer_weight'] = quality.apply(lambda x: x['score_question_weight'] if x['sections'] == 'CUSTOMER' else 0, axis=1)\n",
    "\n",
    "quality['business_score'] = quality.apply(lambda x: x['score_n'] if x['sections'] == 'BUSINESS' else 0, axis=1)\n",
    "quality['business_weight'] = quality.apply(lambda x: x['score_question_weight'] if x['sections'] == 'BUSINESS' else 0, axis=1)\n",
    "\n",
    "quality['compliance_score'] = quality.apply(lambda x: x['score_n'] if x['sections'] == 'COMPLIANCE' else 0, axis=1)\n",
    "quality['compliance_weight'] = quality.apply(lambda x: x['score_question_weight'] if x['sections'] == 'COMPLIANCE' else 0, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# group by quality\n",
    "quality = quality.groupby(['agent_username', 'eval_date'], as_index=False)[['customer_score', 'customer_weight', 'business_score', 'business_weight', 'compliance_score', 'compliance_weight']].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add quality to main frame\n",
    "\n",
    "main_frame = pd.merge(main_frame, quality, left_on=['Booking Login ID', 'Date'], right_on=['agent_username', 'eval_date'], how='left')\n",
    "main_frame[['customer_score', 'customer_weight', 'business_score', 'business_weight', 'compliance_score', 'compliance_weight']] = main_frame[['customer_score', 'customer_weight', 'business_score', 'business_weight', 'compliance_score', 'compliance_weight']].fillna(0)\n",
    "main_frame = main_frame.drop(columns=['agent_username', 'eval_date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change datatype mainframe['Date'] to json\n",
    "main_frame['Date'] = pd.to_datetime(main_frame['Date'], format='mixed').dt.strftime('%Y-%m-%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CSAT\n",
    "if update_less == True:\n",
    "    csat = pd.read_json(os.path.join(r'C:', user_credential, r'DB\\filejson\\csat.json'))\n",
    "else:\n",
    "    csat = pd.read_json(os.path.join(r'C:', user_credential, r'DB\\filejson\\csat_full.json'))\n",
    "    \n",
    "csat['Date'] = csat['Date'].dt.date\n",
    "## get LOB\n",
    "csat = pd.merge(csat, schedule[['Emp ID', 'Date', 'LOB']], left_on=['Employee_ID', 'Date'], right_on=['Emp ID', 'Date'], how='left')\n",
    "csat = csat.drop(columns={'Emp ID'})\n",
    "csat = csat.dropna(subset=['LOB'])\n",
    "\n",
    "# count phone survey\n",
    "def phone_surveycount_overall(x):\n",
    "    if x['Channel'] == 'phone':\n",
    "        if x['LOB'] != 'VICSG':\n",
    "            return 1\n",
    "        else:\n",
    "            if x['Language'] == 'Vietnamese':\n",
    "                return 1\n",
    "            else:\n",
    "                return 0\n",
    "    else:\n",
    "        return 0\n",
    "# count phone csat\n",
    "def phone_csatcount_overall(x):\n",
    "    if x['Channel'] == 'phone':\n",
    "        if x['Csat 2.0 Score'] in ['Satisfied', 'Very Satisfied']:\n",
    "            if x['LOB'] != 'VICSG':\n",
    "                return 1\n",
    "            else:\n",
    "                if x['Language'] == 'Vietnamese':\n",
    "                    return 1\n",
    "                else:\n",
    "                    return 0\n",
    "        else:\n",
    "            return 0\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "# count non phone survey\n",
    "def nonphone_surveycount_overall(x):\n",
    "    if x['Channel'] != 'phone':\n",
    "        if x['LOB'] != 'VICSG':\n",
    "            return 1\n",
    "        else:\n",
    "            if x['Language'] == 'Vietnamese':\n",
    "                return 1\n",
    "            else:\n",
    "                return 0\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "def nonphone_csatcount_overall(x):\n",
    "    if x['Channel'] != 'phone':\n",
    "        if x['Csat 2.0 Score'] in ['Satisfied', 'Very Satisfied']:\n",
    "            if x['LOB'] != 'VICSG':\n",
    "                return 1\n",
    "            else:\n",
    "                if x['Language'] == 'Vietnamese':\n",
    "                    return 1\n",
    "                else:\n",
    "                    return 0\n",
    "        else:\n",
    "            return 0\n",
    "    else:\n",
    "        return 0\n",
    "        \n",
    "csat['phone_survey'] = csat.apply(phone_surveycount_overall, axis=1)\n",
    "csat['nonphone_survey'] = csat.apply(nonphone_surveycount_overall, axis=1)\n",
    "csat['phone_csat'] = csat.apply(phone_csatcount_overall, axis=1)\n",
    "csat['nonphone_csat'] = csat.apply(nonphone_csatcount_overall, axis=1)\n",
    "csat['Date'] = pd.to_datetime(csat['Date'], format='mixed').dt.strftime('%Y-%m-%d')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarise csat\n",
    "csat = csat.groupby(['Employee_ID', 'Date'], as_index=False)[['phone_survey', 'nonphone_survey', 'phone_csat', 'nonphone_csat']].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add csat to main frame\n",
    "main_frame = pd.merge(main_frame, csat, left_on=['Emp ID', 'Date'], right_on=['Employee_ID', 'Date'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CSAT_reso Reso\n",
    "csat_reso = pd.read_json(os.path.join(r'C:', user_credential, r'DB\\filejson\\csat_reso_full.json'))\n",
    "csat_reso['Date'] = csat_reso['Date'].dt.date\n",
    "\n",
    "## get LOB\n",
    "csat_reso = pd.merge(csat_reso, schedule[['Emp ID', 'Date', 'LOB']], left_on=['Employee_ID', 'Date'], right_on=['Emp ID', 'Date'], how='left')\n",
    "csat_reso = csat_reso.drop(columns={'Emp ID'})\n",
    "csat_reso = csat_reso.dropna(subset=['LOB'])\n",
    "\n",
    "# count phone survey\n",
    "def phone_surveycount_overall(x):\n",
    "    if x['Channel'] == 'phone':\n",
    "        if x['LOB'] != 'VICSG':\n",
    "            return 1\n",
    "        else:\n",
    "            if x['Language'] == 'Vietnamese':\n",
    "                return 1\n",
    "            else:\n",
    "                return 0\n",
    "    else:\n",
    "        return 0\n",
    "# count phone csat_reso\n",
    "def phone_csat_resocount_overall(x):\n",
    "    if x['Channel'] == 'phone':\n",
    "        if x['Csat 2.0 Score'] in ['Satisfied', 'Very Satisfied']:\n",
    "            if x['LOB'] != 'VICSG':\n",
    "                return 1\n",
    "            else:\n",
    "                if x['Language'] == 'Vietnamese':\n",
    "                    return 1\n",
    "                else:\n",
    "                    return 0\n",
    "        else:\n",
    "            return 0\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "# count non phone survey\n",
    "def nonphone_surveycount_overall(x):\n",
    "    if x['Channel'] != 'phone':\n",
    "        if x['LOB'] != 'VICSG':\n",
    "            return 1\n",
    "        else:\n",
    "            if x['Language'] == 'Vietnamese':\n",
    "                return 1\n",
    "            else:\n",
    "                return 0\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "def nonphone_csat_resocount_overall(x):\n",
    "    if x['Channel'] != 'phone':\n",
    "        if x['Csat 2.0 Score'] in ['Satisfied', 'Very Satisfied']:\n",
    "            if x['LOB'] != 'VICSG':\n",
    "                return 1\n",
    "            else:\n",
    "                if x['Language'] == 'Vietnamese':\n",
    "                    return 1\n",
    "                else:\n",
    "                    return 0\n",
    "        else:\n",
    "            return 0\n",
    "    else:\n",
    "        return 0\n",
    "        \n",
    "csat_reso['phone_survey_reso'] = csat_reso.apply(phone_surveycount_overall, axis=1)\n",
    "csat_reso['nonphone_survey_reso'] = csat_reso.apply(nonphone_surveycount_overall, axis=1)\n",
    "csat_reso['phone_csat_reso'] = csat_reso.apply(phone_csat_resocount_overall, axis=1)\n",
    "csat_reso['nonphone_csat_reso'] = csat_reso.apply(nonphone_csat_resocount_overall, axis=1)\n",
    "csat_reso['Date'] = pd.to_datetime(csat_reso['Date'], format='mixed').dt.strftime('%Y-%m-%d')\n",
    "\n",
    "# summarise csat_reso\n",
    "csat_reso = csat_reso.groupby(['Employee_ID', 'Date'], as_index=False)[['phone_survey_reso', 'nonphone_survey_reso', 'phone_csat_reso', 'nonphone_csat_reso']].sum()\n",
    "\n",
    "# add csat_reso to main frame\n",
    "main_frame = main_frame.drop(columns={'Employee_ID'})\n",
    "main_frame = pd.merge(main_frame, csat_reso, left_on=['Emp ID', 'Date'], right_on=['Employee_ID', 'Date'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PSAT\n",
    "psat = pd.read_json(os.path.join(r'C:', user_credential, r'DB\\filejson\\psat.json'))\n",
    "\n",
    "def survey_count(x):\n",
    "    if x['Agent understood my question'] != 'No Answer':\n",
    "        q1 = 1\n",
    "    else:\n",
    "        q1 = 0\n",
    "    if x['Agent did everything possible to help me'] != 'No Answer':\n",
    "        q2 = 1\n",
    "    else:\n",
    "        q2 = 0\n",
    "    survey_count = q1 + q2\n",
    "    return survey_count\n",
    "\n",
    "def survey_count_vnese(x):\n",
    "    if x['Agent understood my question'] != 'No Answer' and x['Language'] == 'Vietnamese':\n",
    "        q1 = 1\n",
    "    else:\n",
    "        q1 = 0\n",
    "    if x['Agent did everything possible to help me'] != 'No Answer' and x['Language'] == 'Vietnamese':\n",
    "        q2 = 1\n",
    "    else:\n",
    "        q2 = 0\n",
    "    survey_count = q1 + q2\n",
    "    return survey_count\n",
    "\n",
    "def survey_count_american(x):\n",
    "    if x['Agent understood my question'] != 'No Answer' and x['Language'] == 'English (American)':\n",
    "        q1 = 1\n",
    "    else:\n",
    "        q1 = 0\n",
    "    if x['Agent did everything possible to help me'] != 'No Answer' and x['Language'] == 'English (American)':\n",
    "        q2 = 1\n",
    "    else:\n",
    "        q2 = 0\n",
    "    survey_count = q1 + q2\n",
    "    return survey_count\n",
    "\n",
    "def survey_count_britain(x):\n",
    "    if x['Agent understood my question'] != 'No Answer' and x['Language'] == 'English (Great Britain)':\n",
    "        q1 = 1\n",
    "    else:\n",
    "        q1 = 0\n",
    "    if x['Agent did everything possible to help me'] != 'No Answer' and x['Language'] == 'English (Great Britain)':\n",
    "        q2 = 1\n",
    "    else:\n",
    "        q2 = 0\n",
    "    survey_count = q1 + q2\n",
    "    return survey_count\n",
    "\n",
    "\n",
    "def psat_count(x):\n",
    "    if x['Agent understood my question'] in ['Satisfied', 'Very Satisfied']:\n",
    "        q1 = 1\n",
    "    else:\n",
    "        q1 = 0\n",
    "    if x['Agent did everything possible to help me'] in ['Satisfied', 'Very Satisfied']:\n",
    "        q2 = 1\n",
    "    else:\n",
    "        q2 = 0\n",
    "    psat_count = q1 + q2\n",
    "    return psat_count\n",
    "\n",
    "def psat_count_vnese(x):\n",
    "    if x['Agent understood my question'] in ['Satisfied', 'Very Satisfied'] and x['Language'] == 'Vietnamese':\n",
    "        q1 = 1\n",
    "    else:\n",
    "        q1 = 0\n",
    "    if x['Agent did everything possible to help me'] in ['Satisfied', 'Very Satisfied'] and x['Language'] == 'Vietnamese':\n",
    "        q2 = 1\n",
    "    else:\n",
    "        q2 = 0\n",
    "    psat_count = q1 + q2\n",
    "    return psat_count\n",
    "\n",
    "def psat_count_american(x):\n",
    "    if x['Agent understood my question'] in ['Satisfied', 'Very Satisfied'] and x['Language'] == 'English (American)':\n",
    "        q1 = 1\n",
    "    else:\n",
    "        q1 = 0\n",
    "    if x['Agent did everything possible to help me'] in ['Satisfied', 'Very Satisfied'] and x['Language'] == 'English (American)':\n",
    "        q2 = 1\n",
    "    else:\n",
    "        q2 = 0\n",
    "    psat_count = q1 + q2\n",
    "    return psat_count\n",
    "\n",
    "def psat_count_britain(x):\n",
    "    if x['Agent understood my question'] in ['Satisfied', 'Very Satisfied'] and x['Language'] == 'English (Great Britain)':\n",
    "        q1 = 1\n",
    "    else:\n",
    "        q1 = 0\n",
    "    if x['Agent did everything possible to help me'] in ['Satisfied', 'Very Satisfied'] and x['Language'] == 'English (Great Britain)':\n",
    "        q2 = 1\n",
    "    else:\n",
    "        q2 = 0\n",
    "    psat_count = q1 + q2\n",
    "    return psat_count\n",
    "\n",
    "psat['psat_survey'] = psat.apply(survey_count, axis=1)\n",
    "psat['psat_count'] = psat.apply(psat_count, axis=1)\n",
    "\n",
    "psat['psat_survey_vnese'] = psat.apply(survey_count_vnese, axis=1)\n",
    "psat['psat_count_vnese'] = psat.apply(psat_count_vnese, axis=1)\n",
    "\n",
    "psat['psat_survey_american'] = psat.apply(survey_count_american, axis=1)\n",
    "psat['psat_count_american'] = psat.apply(psat_count_american, axis=1)\n",
    "\n",
    "psat['psat_survey_britain'] = psat.apply(survey_count_britain, axis=1)\n",
    "psat['psat_count_britain'] = psat.apply(psat_count_britain, axis=1)\n",
    "# add ID\n",
    "psat = pd.merge(psat, masterroster[['Employee_ID', 'TED Name']], left_on='Staff Name', right_on='TED Name', how='left')\n",
    "psat = psat.drop(columns=['TED Name'])\n",
    "psat['Date'] = pd.to_datetime(psat['Date'], format='mixed').dt.strftime('%Y-%m-%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarise psat\n",
    "psat = psat.groupby(['Employee_ID', 'Date'], as_index=False)[['psat_survey', 'psat_count', 'psat_survey_vnese', 'psat_count_vnese', 'psat_survey_american', 'psat_count_american', 'psat_survey_britain', 'psat_count_britain']].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add psat to main frame\n",
    "main_frame = pd.merge(main_frame, psat, left_on=['Emp ID', 'Date'], right_on=['Employee_ID', 'Date'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove abundant cols\n",
    "main_frame = main_frame.drop(columns={'Employee_ID_x', 'Employee_ID_y'})\n",
    "# fillna\n",
    "main_frame[['psat_survey', 'psat_count', 'phone_survey', 'nonphone_survey', 'phone_csat', 'nonphone_csat']] = main_frame[['psat_survey', 'psat_count', 'phone_survey', 'nonphone_survey', 'phone_csat', 'nonphone_csat']].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get kpi_target\n",
    "kpi_target = pd.read_json(os.path.join(r'C:', user_credential, r'DB\\filejson\\kpi_target.json'))\n",
    "main_frame['Week'] = main_frame['Week'].astype('int64')\n",
    "main_frame = pd.merge(main_frame, kpi_target, on=['Week', 'Tenure days', 'LOB Group'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get attendance\n",
    "attendance = pd.read_json(os.path.join(r'C:', user_credential, r'DB\\filejson\\booking_attendance.json'))\n",
    "attendance['Date'] = pd.to_datetime(attendance['Date'], format='mixed').dt.strftime('%Y-%m-%d')\n",
    "# add attendance to main frame\n",
    "main_frame = pd.merge(main_frame, attendance[['Emp ID', 'Date', 'Actual Leave', 'Attendance', 'Total scheduled']], on=['Emp ID', 'Date'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get RONA\n",
    "rona = pd.read_json(os.path.join(r'C:', user_credential, r'DB\\filejson\\rona.json'))\n",
    "rona = rona.groupby(['Session Date', 'Employee_ID'], as_index=False)['RONA'].sum()\n",
    "rona['Session Date'] = pd.to_datetime(rona['Session Date'], format='mixed').dt.strftime('%Y-%m-%d')\n",
    "\n",
    "## Add Rona to main frame\n",
    "main_frame = pd.merge(main_frame, rona, left_on=['Emp ID', 'Date'], right_on=['Employee_ID', 'Session Date'], how='left')\n",
    "main_frame = main_frame.drop(columns={'Employee_ID', 'Session Date'})\n",
    "main_frame['RONA'] = main_frame['RONA'].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_frame['PST_Start_Date'] = pd.to_datetime(main_frame['PST_Start_Date'], format='mixed').dt.strftime('%Y-%m-%d')\n",
    "main_frame['Date'] = pd.to_datetime(main_frame['Date'], format='mixed').dt.strftime('%Y-%m-%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export to json\n",
    "if update_less is False:\n",
    "    main_frame.loc[(pd.to_datetime(main_frame['Date'], format='mixed')<time_reset)&(pd.to_datetime(main_frame['Date'], format='mixed')>dt(year=2023, month=1, day=1))].to_json(storage_path, orient='records')\n",
    "else:\n",
    "    main_storage = pd.read_json(storage_path)\n",
    "    main_frame = pd.concat([main_storage, main_frame.loc[pd.to_datetime(main_frame['Date'], format='mixed')>=time_reset]])\n",
    "    main_frame['Date'] = pd.to_datetime(main_frame['Date'], format='mixed').dt.strftime('%Y-%m-%d')\n",
    "    \n",
    "    main_frame.to_csv(output_csv_path, index=False)\n",
    "    main_frame.to_csv(output_tony_path, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
