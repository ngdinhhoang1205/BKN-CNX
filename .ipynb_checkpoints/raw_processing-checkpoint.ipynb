{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "#libraries\n",
    "import os\n",
    "import pandas as pd\n",
    "from datetime import datetime as dt\n",
    "from datetime import time as t\n",
    "import numpy as np\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOURCE\n",
    "\n",
    "## PLEASE INPUT YOUR OWN CREDENTIAL BELOW\n",
    "user_credential = r'/Users/dinhhoang.nguyen.CONCENTRIX/OneDrive - Concentrix Corporation/WFM-internal'\n",
    "\n",
    "eps_path = os.path.join(\"C:\", user_credential, r\"Data\\EPS Tableau\\EPS Query\")\n",
    "cpi_path = os.path.join(\"C:\", user_credential, r\"Data\\Ticket\\CPI Query\")\n",
    "aht_path = os.path.join(\"C:\", user_credential, r\"Data\\AHT\\AHT query\")\n",
    "csat_path = os.path.join(\"C:\", user_credential, r\"Data\\CSAT RAW\\CSAT Query\")\n",
    "csatreso_path = os.path.join(\"C:\", user_credential, r\"Data\\CSAT Reso\\CSAT Reso Query\")\n",
    "cuic_path = os.path.join(\"C:\", user_credential, r\"Data\\CUIC\\CUIC Query\")\n",
    "exception_path = os.path.join(\"C:\", user_credential, r\"Data\\Exception\")\n",
    "extension_path = os.path.join(\"C:\", user_credential, r\"Data\\Extension\")\n",
    "iex_path = os.path.join(\"C:\", user_credential, r\"Data\\IEX\")\n",
    "label_path = os.path.join(\"C:\", user_credential, r\"Data\\Labels\")\n",
    "ioshrinkage_path = os.path.join(\"C:\", user_credential, r\"Data\\IO Shrinkage\")\n",
    "ot_path = os.path.join(\"C:\", user_credential, r\"Data\\OT\\OT Query\")\n",
    "psat_path = os.path.join(\"C:\", user_credential, r\"Data\\PSAT RAW\\PSAT Query\")\n",
    "quality_path = os.path.join(\"C:\", user_credential, r\"Data\\Quality.v2\\Quality query\")\n",
    "ramco_path = os.path.join(\"C:\", user_credential, r\"Data\\Ramco\\Ramco Query\")\n",
    "ramcoot_path = os.path.join(\"C:\", user_credential, r\"Data\\Ramco OT\\Ramco OT Query\")\n",
    "requirement_path = os.path.join(\"C:\", user_credential, r\"Data\\Requirement\")\n",
    "schedule_path = os.path.join(\"C:\", user_credential, r\"Data\\Schedule 2\\Schedule Query\")\n",
    "tl_path = os.path.join(\"C:\", user_credential, r\"Data\\TL\")\n",
    "training_path = os.path.join(\"C:\", user_credential, r\"Data\\Training\")\n",
    "masterroster_path = os.path.join(\"C:\", user_credential, r\"Data\\User\")\n",
    "workplan_path = os.path.join(\"C:\", user_credential, r\"Data\\Workplan\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FUNCTION TO IMPORT FOLDERS\n",
    "def import_csv(path):\n",
    "    raw = []\n",
    "    for file in os.listdir(path):\n",
    "        if file.endswith(\".csv\"):\n",
    "            file_path = os.path.join(path, file)\n",
    "            file_data = pd.read_csv(file_path)\n",
    "            file_data[\"filename\"] = file\n",
    "            raw.append(file_data)\n",
    "            final_raw = pd.concat(raw, ignore_index=True)\n",
    "    return final_raw\n",
    "    \n",
    "def import_xlsx(path, sheet_name):\n",
    "    raw = []\n",
    "    for file in os.listdir(path):\n",
    "        if file.endswith(\".xlsx\"):\n",
    "            file_path = os.path.join(path, file)\n",
    "            file_data = pd.read_excel(file_path, engine=\"openpyxl\", sheet_name = sheet_name)\n",
    "            file_data[\"filename\"] = file\n",
    "            raw.append(file_data)\n",
    "            final_raw = pd.concat(raw, ignore_index=True)\n",
    "    return final_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORT MASTER ROSTER\n",
    "masterroster = pd.read_excel(r\"C:\\Users\\dinhhoang.nguyen.CONCENTRIX\\OneDrive - Concentrix Corporation\\WFM-internal\\Data\\User\\CNX Global Master Roster.xlsx\", sheet_name=\"Sheet1\")\n",
    "masterroster['Employee_ID'] = masterroster['Employee_ID'].astype(\"Int64\")\n",
    "# export master roster to json\n",
    "masterroster['PST_Start_Date'] = pd.to_datetime(masterroster['PST_Start_Date'], format='mixed').dt.strftime('%Y-%m-%d')\n",
    "masterroster.to_json(os.path.join(r'C:', user_credential, r'DB\\filejson\\masterroster.json'), orient='records')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORT EPS\n",
    "eps = import_csv(path = eps_path)\n",
    "eps = eps.drop(columns=['Index', 'filename', 'Date'])\n",
    "eps['Session login VN'] = pd.to_datetime(eps['Session Login'], format ='mixed') + pd.Timedelta(hours=5)\n",
    "eps['Session logout VN'] = pd.to_datetime(eps['Session Logout'], format ='mixed') + pd.Timedelta(hours=5)\n",
    "eps['Session login date'] = pd.to_datetime(eps['Session login VN']).dt.strftime('%Y-%m-%d')\n",
    "eps['Session login time'] = pd.to_datetime(eps['Session login VN']).dt.time\n",
    "eps['Session logout date'] = pd.to_datetime(eps['Session logout VN']).dt.strftime('%Y-%m-%d')\n",
    "eps['Session logout time'] = pd.to_datetime(eps['Session logout VN']).dt.time\n",
    "eps = eps.drop(columns=['Session login VN', 'Session logout VN', 'Username', 'manager_username', 'sitecode', 'Session Time'])\n",
    "eps.to_csv(r\"C:\\Users\\dinhhoang.nguyen\\OneDrive - Concentrix Corporation\\WFM-internal\\DB\\filecsv\\eps_test.csv\")\n",
    "\n",
    "# export eps to json\n",
    "eps.to_json(os.path.join(r'C:', user_credential, r'DB\\filejson\\eps.json'), orient='records')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "#IMPORT CPI\n",
    "cpi = import_csv(path = cpi_path)\n",
    "cpi = cpi.drop(columns=['Region Partner', 'Sitecode', 'Mp Csm Name', 'Mp Team Name'])\n",
    "cpi['Date'] = cpi['filename'].replace(\".csv\", \"\", regex=True)\n",
    "cpi['Date'] = pd.to_datetime(cpi['Date'], format = \"mixed\")\n",
    "\n",
    "masterroster_cpi = masterroster[['Employee_ID', 'TED Name']]\n",
    "cpi = pd.merge(cpi, masterroster_cpi, left_on=\"Staff Name\", right_on=\"TED Name\", how=\"left\")\n",
    "cpi = cpi.drop(columns=['filename', 'TED Name'])\n",
    "cpi = cpi[['Staff Name', 'Employee_ID', 'Date', 'Channel', 'Number of Records']]\n",
    "\n",
    "# export cpi to json\n",
    "cpi.to_json(os.path.join(r'C:', user_credential, r'DB\\filejson\\cpi.json'), orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dinhhoang.nguyen.CONCENTRIX\\AppData\\Local\\Temp\\VSCodePortable-x64Temp\\ipykernel_16788\\2378756514.py:7: DtypeWarning: Columns (10,11) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  file_data = pd.read_csv(file_path)\n",
      "C:\\Users\\dinhhoang.nguyen.CONCENTRIX\\AppData\\Local\\Temp\\VSCodePortable-x64Temp\\ipykernel_16788\\2378756514.py:7: DtypeWarning: Columns (10,11) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  file_data = pd.read_csv(file_path)\n",
      "C:\\Users\\dinhhoang.nguyen.CONCENTRIX\\AppData\\Local\\Temp\\VSCodePortable-x64Temp\\ipykernel_16788\\2378756514.py:7: DtypeWarning: Columns (0,1,2,3,4,5,6,7,8,12) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  file_data = pd.read_csv(file_path)\n"
     ]
    }
   ],
   "source": [
    "#IMPORT AHT\n",
    "aht = import_csv(path = aht_path)\n",
    "aht = aht.drop(columns=['filename'])\n",
    "masterroster_aht = masterroster[['Employee_ID', 'TED Name']]\n",
    "aht = pd.merge(aht, masterroster_aht, left_on='Staff', right_on ='TED Name', how='left')\n",
    "aht = aht.drop(columns=['TED Name'])\n",
    "aht['Date'] = pd.to_datetime(aht['Date'], format='mixed')\n",
    "\n",
    "# export aht to json\n",
    "aht.to_json(os.path.join(r'C:', user_credential, r'DB\\filejson\\aht.json'), orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "#IMPORT CSAT\n",
    "csat = import_csv(path = csat_path)\n",
    "masterroster_csat = masterroster[['Employee_ID', 'TED Name']]\n",
    "csat['Date '] = pd.to_datetime(csat['Date '], format='%m/%d/%Y')\n",
    "csat = csat.rename(columns={'Date ': 'Date'})\n",
    "csat = pd.merge(csat, masterroster_csat, left_on='Staff', right_on ='TED Name', how='left')\n",
    "csat = csat.drop(columns=['Sort by Dimension', 'Has Comment', '\"Comment\"', 'Reservation Link', 'View comment', 'Sort by Dimension (copy)', 'filename', 'TED Name', 'Max. Sort by Dimension'])\n",
    "csat['Date'] = csat['Date'].dt.strftime('%Y-%m-%d')\n",
    "#export csat to json\n",
    "csat.to_json(os.path.join(r'C:', user_credential, r'DB\\filejson\\csat.json'), orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "#IMPORT CSAT_reso\n",
    "csat_reso = import_csv(path = csatreso_path)\n",
    "masterroster_csat_reso = masterroster[['Employee_ID', 'TED Name']]\n",
    "csat_reso['Date '] = pd.to_datetime(csat_reso['Date '], format='mixed')\n",
    "csat_reso = csat_reso.rename(columns={'Date ': 'Date'})\n",
    "csat_reso = pd.merge(csat_reso, masterroster_csat_reso, left_on='Staff', right_on ='TED Name', how='left')\n",
    "csat_reso = csat_reso.drop(columns=['Sort by Dimension', 'Has Comment', '\"Comment\"', 'Reservation Link', 'View comment', 'Sort by Dimension (copy)', 'filename', 'TED Name', 'Max. Sort by Dimension'])\n",
    "csat_reso['Date'] = csat_reso['Date'].dt.strftime('%Y-%m-%d')\n",
    "#export csat_reso to json\n",
    "csat_reso.to_json(os.path.join(r'C:', user_credential, r'DB\\filejson\\csat_reso.json'), orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORT RAMCO\n",
    "ramco = import_csv(path = ramco_path)\n",
    "ramco['Attribute'] = pd.to_datetime(ramco['Attribute'],format='mixed').dt.date\n",
    "ramco=ramco.rename(columns={'Attribute':'Date'})\n",
    "ramco['EID']=ramco['EID'].astype('Int64')\n",
    "\n",
    "# export ramco to json\n",
    "ramco.to_json(os.path.join(r'C:', user_credential, r'DB\\filejson\\ramco.json'), orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dinhhoang.nguyen.CONCENTRIX\\AppData\\Local\\Temp\\VSCodePortable-x64Temp\\ipykernel_16788\\3985041532.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  week_shift['Date'] = pd.to_datetime(week_shift['Date'],errors='coerce')\n",
      "C:\\Users\\dinhhoang.nguyen.CONCENTRIX\\AppData\\Local\\Temp\\VSCodePortable-x64Temp\\ipykernel_16788\\3985041532.py:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  week_shift['Week'] = week_shift['Date'].dt.strftime('%Y%W')\n"
     ]
    }
   ],
   "source": [
    "# IMPORT SCHEDULE\n",
    "schedule = import_xlsx(path = schedule_path,sheet_name='Sheet1')\n",
    "schedule['Attribute'] = pd.to_datetime(schedule['Attribute'],format='mixed').dt.date\n",
    "schedule=schedule.iloc[:,0:8]\n",
    "condition=(schedule['Value']=='OFF')|(schedule['Value']=='AL')|(schedule['Value']=='CO')|(schedule['Value']=='UPL')|(schedule['Value']=='VGH')\n",
    "condition_training=(schedule['Value']=='Training')|(schedule['Value']=='PEGA')\n",
    "schedule['Shift'] = np.where(condition, 'OFF', schedule['NS Check'])\n",
    "schedule['Shift'] = np.where(condition_training, 'Training', schedule['Shift'])\n",
    "schedule=schedule[['Emp ID','Name','Attribute','Value','LOB','Shift']]\n",
    "schedule=schedule.rename(columns={'Attribute':'Date','Value':'Shift','Shift':'Shift_type'})\n",
    "schedule['Emp ID'] = schedule['Emp ID'].astype(\"Int64\")\n",
    " \n",
    "week_shift=schedule[['Emp ID','Date','Shift']]\n",
    "week_shift['Date'] = pd.to_datetime(week_shift['Date'],errors='coerce')\n",
    "week_shift['Week'] = week_shift['Date'].dt.strftime('%Y%W')\n",
    "week_shift=week_shift[['Emp ID','Week','Shift']]\n",
    "week_shift = week_shift[week_shift['Shift'].str.contains('00$')]\n",
    "week_shift=week_shift.groupby(['Emp ID','Week','Shift'],as_index=False).count()\n",
    "week_shift=week_shift.groupby(['Emp ID','Week'],as_index=False).agg(min=('Shift','min'))\n",
    "week_shift=week_shift.rename(columns={'min':'Week_shift'})\n",
    " \n",
    " \n",
    "schedule['Date'] = pd.to_datetime(schedule['Date'],errors='coerce')\n",
    "schedule['Week'] = schedule['Date'].dt.strftime('%Y%W')\n",
    "schedule=pd.merge(schedule,week_shift,left_on=['Emp ID','Week'],right_on=['Emp ID','Week'],how='left')\n",
    "schedule['Date']=pd.to_datetime(schedule['Date'],format='mixed').dt.strftime('%Y-%m-%d')\n",
    "schedule=pd.merge(schedule,ramco,left_on=['Emp ID','Date'],right_on=['EID','Date'],how='left')\n",
    "def actualshift_func(x):\n",
    "    if x['Value']=='PH' or x['Value']=='PO' :\n",
    "        return x['Week_shift']\n",
    "    else:         \n",
    "        return x['Shift']\n",
    "schedule['Actual_shift'] = schedule.apply(actualshift_func, axis=1)\n",
    "def actualshift1_func(x):\n",
    "    if x['Actual_shift'] in ['0400-1300','0500-1400','0600-1500','0700-1600','0800-1700','0900-1800','1000-1900','1100-2000',\n",
    "                      '1200-2100','1300-2200','1400-2300','1500-2400','1600-0100','1700-0200', '1800-0300','1900-0400']:     \n",
    "        return 'DS'\n",
    "    elif x['Actual_shift'] in ['2000-0500','2100-0600','2200-0700']:\n",
    "        return 'NS'\n",
    "    else:      \n",
    "        return x['Shift_type']\n",
    "schedule['Actual_shift_type'] = schedule.apply(actualshift1_func, axis=1)\n",
    "schedule=schedule[['Emp ID','Name','Date','Actual_shift','LOB','Actual_shift_type']]\n",
    "schedule=schedule.rename(columns={'Actual_shift':'Shift','Actual_shift_type':'Shift_type'})\n",
    "# export schedule to json\n",
    "schedule.to_json(os.path.join(r'C:', user_credential, r'DB\\filejson\\schedule.json'), orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_abc = schedule.loc[schedule['Shift_type']=='Training']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Shift_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2023-07-31</td>\n",
       "      <td>DS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2023-08-01</td>\n",
       "      <td>OFF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2023-08-02</td>\n",
       "      <td>DS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2023-08-03</td>\n",
       "      <td>DS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2023-08-04</td>\n",
       "      <td>DS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42672</th>\n",
       "      <td>2023-11-01</td>\n",
       "      <td>Training</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42673</th>\n",
       "      <td>2023-11-02</td>\n",
       "      <td>Training</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42674</th>\n",
       "      <td>2023-11-03</td>\n",
       "      <td>Training</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42675</th>\n",
       "      <td>2023-11-04</td>\n",
       "      <td>OFF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42676</th>\n",
       "      <td>2023-11-05</td>\n",
       "      <td>OFF</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>42677 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             Date Shift_type\n",
       "0      2023-07-31         DS\n",
       "1      2023-08-01        OFF\n",
       "2      2023-08-02         DS\n",
       "3      2023-08-03         DS\n",
       "4      2023-08-04         DS\n",
       "...           ...        ...\n",
       "42672  2023-11-01   Training\n",
       "42673  2023-11-02   Training\n",
       "42674  2023-11-03   Training\n",
       "42675  2023-11-04        OFF\n",
       "42676  2023-11-05        OFF\n",
       "\n",
       "[42677 rows x 2 columns]"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "schedule[['Date', 'Shift_type']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "#IMPORT CUIC\n",
    "cuic = import_xlsx(path = cuic_path,sheet_name=\"Khoa Interval Attendance Check-\")\n",
    "cuic['Date'] = pd.to_datetime(cuic['Interval'], format = \"mixed\").dt.date\n",
    "cuic['Time'] = pd.to_datetime(cuic['Interval'], format = \"mixed\").dt.time\n",
    "cuic = pd.merge(cuic, masterroster, left_on='LoginName', right_on ='Booking Login ID', how='left')\n",
    "cuic=cuic.iloc[:,0:9]\n",
    "\n",
    "schedule['Date'] = pd.to_datetime(schedule['Date'], format='mixed').dt.date\n",
    "\n",
    "cuic = pd.merge(cuic, schedule, left_on=['Employee_ID','Date'], right_on =['Emp ID','Date'], how='left')\n",
    "cuic=cuic[['FullName','LoginName','Interval','AgentAvailTime','AgentLoggedOnTime','Date','Time','Employee_ID','Shift_type','Shift']]\n",
    "cuic['Date-1']=pd.to_datetime(cuic['Date'], format ='mixed') - pd.Timedelta(1,\"d\")\n",
    "cuic['Date-1']=pd.to_datetime(cuic['Date-1'], format ='mixed').dt.date\n",
    "cuic = pd.merge(cuic, schedule, left_on=['Employee_ID','Date-1'], right_on =['Emp ID','Date'], how='left')\n",
    "cuic=cuic[['FullName','LoginName','Interval','AgentAvailTime','AgentLoggedOnTime','Date_x','Time','Employee_ID','Shift_type_x','Shift_x','Date-1','Shift_type_y','Shift_y']]\n",
    "mytime=t(12,0,0)\n",
    "condition=(cuic['Shift_type_x']!='DS')&(cuic['Shift_type_y']=='NS')&(cuic['Time']<mytime)\n",
    "cuic['Session Date'] = np.where(condition, cuic['Date-1'], cuic['Date_x'])\n",
    "cuic=cuic[['FullName','LoginName','Interval','AgentAvailTime','AgentLoggedOnTime','Date_x','Employee_ID','Shift_type_x','Session Date']]\n",
    "cuic=cuic.rename(columns={'Date_x':'Date','Shift_type_x':'Shift_type'})\n",
    "cuic['Session Date'] = pd.to_datetime(cuic['Session Date'], format='mixed').dt.strftime('%Y-%m-%d')\n",
    "# export cuic to json\n",
    "cuic.to_json(os.path.join(r'C:', user_credential, r'DB\\filejson\\cuic.json'), orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FullName</th>\n",
       "      <th>LoginName</th>\n",
       "      <th>Interval</th>\n",
       "      <th>AgentAvailTime</th>\n",
       "      <th>AgentLoggedOnTime</th>\n",
       "      <th>Date</th>\n",
       "      <th>Employee_ID</th>\n",
       "      <th>Shift_type</th>\n",
       "      <th>Session Date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>558</th>\n",
       "      <td>Luu, John</td>\n",
       "      <td>jluu</td>\n",
       "      <td>8/1/23 10:00:00 AM</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.003113</td>\n",
       "      <td>2023-08-01</td>\n",
       "      <td>101993777</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2023-08-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>953</th>\n",
       "      <td>Nguyen, Truyen</td>\n",
       "      <td>tnguyen38</td>\n",
       "      <td>8/1/23 8:30:00 AM</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.005394</td>\n",
       "      <td>2023-08-01</td>\n",
       "      <td>102216751</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2023-08-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>954</th>\n",
       "      <td>Nguyen, Truyen</td>\n",
       "      <td>tnguyen38</td>\n",
       "      <td>8/1/23 9:00:00 AM</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.007535</td>\n",
       "      <td>2023-08-01</td>\n",
       "      <td>102216751</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2023-08-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>955</th>\n",
       "      <td>Nguyen, Truyen</td>\n",
       "      <td>tnguyen38</td>\n",
       "      <td>8/1/23 9:30:00 AM</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.001782</td>\n",
       "      <td>2023-08-01</td>\n",
       "      <td>102216751</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2023-08-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1081</th>\n",
       "      <td>Pham, Krist</td>\n",
       "      <td>kpham6</td>\n",
       "      <td>8/1/23 7:00:00 AM</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.011204</td>\n",
       "      <td>2023-08-01</td>\n",
       "      <td>102261567</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2023-08-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>532189</th>\n",
       "      <td>Pham, Joey</td>\n",
       "      <td>jpham13</td>\n",
       "      <td>10/31/23 1:00:00 PM</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.011574</td>\n",
       "      <td>2023-10-31</td>\n",
       "      <td>102198985</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2023-10-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>532190</th>\n",
       "      <td>Pham, Joey</td>\n",
       "      <td>jpham13</td>\n",
       "      <td>10/31/23 1:30:00 PM</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.004468</td>\n",
       "      <td>2023-10-31</td>\n",
       "      <td>102198985</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2023-10-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>532191</th>\n",
       "      <td>Pham, Joey</td>\n",
       "      <td>jpham13</td>\n",
       "      <td>10/31/23 2:00:00 PM</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.002708</td>\n",
       "      <td>2023-10-31</td>\n",
       "      <td>102198985</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2023-10-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>532805</th>\n",
       "      <td>Tran, Yin</td>\n",
       "      <td>ytran2</td>\n",
       "      <td>10/31/23 9:00:00 AM</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.002454</td>\n",
       "      <td>2023-10-31</td>\n",
       "      <td>102003097</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2023-10-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>532972</th>\n",
       "      <td>Vu, Billy</td>\n",
       "      <td>bvu</td>\n",
       "      <td>10/31/23 7:30:00 AM</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000255</td>\n",
       "      <td>2023-10-31</td>\n",
       "      <td>102234790</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2023-10-31</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6601 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              FullName  LoginName             Interval  AgentAvailTime  \\\n",
       "558          Luu, John       jluu   8/1/23 10:00:00 AM             0.0   \n",
       "953     Nguyen, Truyen  tnguyen38    8/1/23 8:30:00 AM             0.0   \n",
       "954     Nguyen, Truyen  tnguyen38    8/1/23 9:00:00 AM             0.0   \n",
       "955     Nguyen, Truyen  tnguyen38    8/1/23 9:30:00 AM             0.0   \n",
       "1081       Pham, Krist     kpham6    8/1/23 7:00:00 AM             0.0   \n",
       "...                ...        ...                  ...             ...   \n",
       "532189      Pham, Joey    jpham13  10/31/23 1:00:00 PM             0.0   \n",
       "532190      Pham, Joey    jpham13  10/31/23 1:30:00 PM             0.0   \n",
       "532191      Pham, Joey    jpham13  10/31/23 2:00:00 PM             0.0   \n",
       "532805       Tran, Yin     ytran2  10/31/23 9:00:00 AM             0.0   \n",
       "532972       Vu, Billy        bvu  10/31/23 7:30:00 AM             0.0   \n",
       "\n",
       "        AgentLoggedOnTime        Date  Employee_ID Shift_type Session Date  \n",
       "558              0.003113  2023-08-01    101993777        NaN   2023-08-01  \n",
       "953              0.005394  2023-08-01    102216751        NaN   2023-08-01  \n",
       "954              0.007535  2023-08-01    102216751        NaN   2023-08-01  \n",
       "955              0.001782  2023-08-01    102216751        NaN   2023-08-01  \n",
       "1081             0.011204  2023-08-01    102261567        NaN   2023-08-01  \n",
       "...                   ...         ...          ...        ...          ...  \n",
       "532189           0.011574  2023-10-31    102198985        NaN   2023-10-31  \n",
       "532190           0.004468  2023-10-31    102198985        NaN   2023-10-31  \n",
       "532191           0.002708  2023-10-31    102198985        NaN   2023-10-31  \n",
       "532805           0.002454  2023-10-31    102003097        NaN   2023-10-31  \n",
       "532972           0.000255  2023-10-31    102234790        NaN   2023-10-31  \n",
       "\n",
       "[6601 rows x 9 columns]"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cuic.loc[cuic['Shift_type'].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "#EXCEPTION\n",
    "exceptional_hrs = import_xlsx(path= exception_path, sheet_name=\"Sheet1\")\n",
    "exceptional_hrs = exceptional_hrs.rename(columns={'Date \\n(MM/DD/YYYY)':'Date'})\n",
    "exceptional_hrs = exceptional_hrs.drop(columns=['filename'])\n",
    "\n",
    "#export exceptional hours to json\n",
    "exceptional_hrs.to_json(os.path.join(r'C:', user_credential, r'DB\\filejson\\exceptional_hrs.json'), orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "#IMPORT IEX ID\n",
    "iex = import_xlsx(path = iex_path, sheet_name=\"Sheet1\")\n",
    "iex = iex[['ID Agent', 'Personal ID']]\n",
    "iex = iex.rename(columns={'Personal ID': 'EID'})\n",
    "\n",
    "#export iex to json\n",
    "iex.to_json(os.path.join(r'C:', user_credential, r'DB\\filejson\\iex.json'), orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORT TL\n",
    "tl = pd.read_excel(os.path.join(tl_path, \"TL List.xlsx\"),sheet_name='Sheet1')\n",
    "tl=tl.iloc[:,0:6]\n",
    "tl=tl.rename(columns={'Supervisor (no need fill)': 'Supervisor'})\n",
    "tl=tl.drop(columns=['Wave', 'Name', 'LOB', 'Full name (VN)'])\n",
    "\n",
    "# export tl to json\n",
    "tl.to_json(os.path.join(r'C:', user_credential, r'DB\\filejson\\tl.json'), orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORT TRAINING\n",
    "training = import_xlsx(path = training_path,sheet_name='Sheet1')\n",
    "training['Date'] = pd.to_datetime(training['Date'],format='mixed').dt.date\n",
    "training['EID'] = training['EID'].astype(\"int64\")\n",
    "training=training.iloc[:,1:]\n",
    "\n",
    "#export training to json\n",
    "training.to_json(os.path.join(r'C:', user_credential, r'DB\\filejson\\training.json'), orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORT IOSHRINKAGE\n",
    "ioshrinkage = import_xlsx(path = ioshrinkage_path, sheet_name='Sheet1')\n",
    "\n",
    "#export ioshrinkage to json\n",
    "ioshrinkage.to_json(os.path.join(r'C:', user_credential, r'DB\\filejson\\ioshrinkage.json'), orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORT OT\n",
    "ot = import_xlsx(path = ot_path, sheet_name='Sheet2')\n",
    "ot = ot.drop(columns=['Wave', 'Name', 'Value', 'LOB', 'filename'])\n",
    "ot['Date'] = pd.to_datetime(ot['Date'], format=\"mixed\")\n",
    "ot['Date'] = ot['Date'].dt.date\n",
    "\n",
    "#export ot to json\n",
    "ot.to_json(os.path.join(r'C:', user_credential, r'DB\\filejson\\ot.json'), orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORT PSAT\n",
    "psat = import_csv(path = psat_path)\n",
    "psat['Date'] = pd.to_datetime(psat['Date'],format='mixed').dt.date\n",
    "\n",
    "#export psat to json\n",
    "psat.to_json(os.path.join(r'C:', user_credential, r'DB\\filejson\\psat.json'), orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORT QUALITY\n",
    "quality_raw = import_xlsx(path = quality_path,sheet_name='Sheet1')\n",
    "quality_raw['eval_date'] = pd.to_datetime(quality_raw['eval_date'],format='mixed').dt.date\n",
    "quality=quality_raw.iloc[:,0:20]\n",
    "quality['eval_date'] = pd.to_datetime(quality['eval_date'], format='mixed').dt.strftime('%Y-%m-%d')\n",
    "#export quality to json\n",
    "quality.to_json(os.path.join(r'C:', user_credential, r'DB\\filejson\\quality.json'), orient='records')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORT RAMCOOT\n",
    "ramcoot = import_xlsx(path = ramcoot_path,sheet_name='Sheet1')\n",
    "ramcoot['Attribute'] = pd.to_datetime(ramcoot['Attribute'],format='mixed').dt.date\n",
    "ramcoot=ramcoot.rename(columns={'Attribute':'Date'})\n",
    "\n",
    "#export ramcoot to json\n",
    "ramcoot.to_json(os.path.join(r'C:', user_credential, r'DB\\filejson\\ramcoot.json'), orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORT REQUIREMENT\n",
    "requirement = import_xlsx(path = requirement_path,sheet_name='Sheet1')\n",
    "requirement['Date'] = pd.to_datetime(requirement['Date'],format='mixed').dt.date\n",
    "\n",
    "#export requirement to json\n",
    "requirement.to_json(os.path.join(r'C:', user_credential, r'DB\\filejson\\requirement.json'), orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORT WORKPLAN\n",
    "workplan_raw = import_xlsx(path = workplan_path,sheet_name='Sheet1')\n",
    "workplan_raw['Date'] = pd.to_datetime(workplan_raw['Date'],format='mixed').dt.date\n",
    "workplan_merge_iex = pd.merge(workplan_raw,iex, left_on='Agent ID', right_on ='ID Agent', how='left')\n",
    "workplan_merge_iex['EID'] = workplan_merge_iex['EID'].astype(\"Int64\")\n",
    "workplan_merge_iex=workplan_merge_iex.drop(columns=['filename', 'ID Agent'])\n",
    "\n",
    "#export workplan_merge_iex to json\n",
    "workplan_merge_iex.to_json(os.path.join(r'C:', user_credential, r'DB\\filejson\\workplan_merge_iex.json'), orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "#IMPORT LABELS\n",
    "labels = import_csv(label_path)\n",
    "\n",
    "#export labels to json\n",
    "labels.to_json(os.path.join(r'C:', user_credential, r'DB\\filejson\\labels.json'), orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "kpi_target = pd.read_excel(os.path.join(r'C:', user_credential, r'Data\\KPI Target\\kpi_target.xlsx'), engine=\"openpyxl\", sheet_name = 'Sheet1')\n",
    "\n",
    "kpi_target.to_json(os.path.join(r'C:', user_credential, r'DB\\filejson\\kpi_target.json'), orient='records')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
